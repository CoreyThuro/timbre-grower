{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# DDSP Timbre Grower - Optimized Training\n\n**Optimizations implemented:**\n1. ‚úÖ Vectorized synthesis (10-50x speedup)\n2. ‚úÖ Reduced FFT scales (2x speedup)\n3. ‚úÖ Segment-based training (3-4x speedup)\n4. ‚úÖ Batching (2-3x speedup)\n5. ‚úÖ Mixed precision training (2-3x speedup on A100)\n6. ‚úÖ Harmonic diversity loss (prevents sine-tone collapse)\n7. ‚úÖ Multi-file training support\n8. ‚úÖ NCA integration for organic harmonic evolution\n9. ‚úÖ Fixed NCA gradient flow\n\n**Expected training time on A100**: 2-5 minutes (quality-tuned)\n**Expected training time on CPU**: 60-90 minutes\n\n**Runtime**: Enable GPU in Colab for best performance!\n\n**Workflow**:\n1. Upload target audio files (single or multiple)\n2. Install dependencies\n3. Define DDSP components\n4. Train DDSP model (~2-3 min on A100, ~60 min on CPU)\n5. Train NCA controller (~30 sec on A100, ~5 min on CPU)\n6. Generate NCA-controlled growing stages\n7. Download results\n\n**Important**: Training is slower than before for quality - the model needs time to learn rich harmonic content, not just the fundamental frequency."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch librosa soundfile matplotlib scipy tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal as scipy_signal\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Check for GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Target Audio Files\n",
    "\n",
    "Upload one or more audio files. Multi-file training improves generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get uploaded filenames\n",
    "audio_paths = list(uploaded.keys())\n",
    "print(f\"Uploaded {len(audio_paths)} file(s):\")\n",
    "for path in audio_paths:\n",
    "    print(f\"  - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "**Training optimization parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training configuration\nSAMPLE_RATE = 22050\nHOP_LENGTH = 512\n\n# OPTIMIZATION 1: Segment-based training\nSEGMENT_DURATION = 1.0  # seconds (reduced from full audio)\nSEGMENT_FRAMES = int((SEGMENT_DURATION * SAMPLE_RATE) / HOP_LENGTH)  # ~43 frames\n\n# OPTIMIZATION 2: Batching (optimized for stability)\nBATCH_SIZE = 8  # Balanced for GPU utilization and training stability\n\n# Model architecture\nN_HARMONICS = 64\nN_FILTER_BANKS = 64\nHIDDEN_SIZE = 512\nN_MFCC = 30\n\n# Training hyperparameters (tuned for quality)\nDDSP_EPOCHS = 3000  # Increased for better convergence\nDDSP_LR = 1e-4  # Reduced for stable learning\n\nNCA_EPOCHS = 500\nNCA_STEPS = 32  # Number of NCA evolution steps\nNCA_LR = 1e-3\n\n# OPTIMIZATION 3: Mixed precision training for A100\nUSE_AMP = True  # Automatic Mixed Precision for 2-3x speedup\n\nprint(f\"Configuration:\")\nprint(f\"  Segment duration: {SEGMENT_DURATION}s ({SEGMENT_FRAMES} frames)\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  DDSP epochs: {DDSP_EPOCHS}\")\nprint(f\"  DDSP learning rate: {DDSP_LR}\")\nprint(f\"  NCA epochs: {NCA_EPOCHS}\")\nprint(f\"  Mixed precision: {USE_AMP}\")\nprint(f\"\\n‚ö†Ô∏è  Training time: ~2-3 minutes (slower for better quality)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DDSP Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class HarmonicOscillator(nn.Module):\n    \"\"\"Differentiable harmonic oscillator for additive synthesis (VECTORIZED).\"\"\"\n\n    def __init__(self, sample_rate=22050, n_harmonics=64):\n        super().__init__()\n        self.sample_rate = sample_rate\n        self.n_harmonics = n_harmonics\n\n    def forward(self, f0_hz, harmonic_amplitudes):\n        batch_size, n_frames = f0_hz.shape\n        hop_length = 512\n        n_samples = n_frames * hop_length\n\n        # Upsample f0 and amplitudes\n        f0_upsampled = F.interpolate(\n            f0_hz.unsqueeze(1), size=n_samples, mode='linear', align_corners=True\n        ).squeeze(1)\n\n        harmonic_amplitudes_upsampled = F.interpolate(\n            harmonic_amplitudes.transpose(1, 2), size=n_samples,\n            mode='linear', align_corners=True\n        ).transpose(1, 2)\n\n        # Compute phase\n        phase = 2 * torch.pi * torch.cumsum(f0_upsampled / self.sample_rate, dim=1)\n\n        # VECTORIZED: Generate all harmonics at once\n        harmonic_numbers = torch.arange(1, self.n_harmonics + 1, device=phase.device).view(1, 1, -1)\n        harmonic_phases = phase.unsqueeze(-1) * harmonic_numbers\n        \n        # Generate all harmonic signals at once\n        harmonic_signals = torch.sin(harmonic_phases)\n        harmonic_signals = harmonic_signals * harmonic_amplitudes_upsampled\n        \n        # Sum all harmonics\n        audio = harmonic_signals.sum(dim=-1)\n\n        return audio\n\n\nclass FilteredNoiseGenerator(nn.Module):\n    \"\"\"Differentiable filtered noise generator (VECTORIZED).\"\"\"\n\n    def __init__(self, sample_rate=22050, n_filter_banks=64):\n        super().__init__()\n        self.sample_rate = sample_rate\n        self.n_filter_banks = n_filter_banks\n\n        self.register_buffer(\n            'filter_freqs',\n            torch.logspace(torch.log10(torch.tensor(20.0)), torch.log10(torch.tensor(sample_rate / 2.0)), n_filter_banks)\n        )\n\n    def forward(self, filter_magnitudes):\n        batch_size, n_frames, _ = filter_magnitudes.shape\n        hop_length = 512\n        n_samples = n_frames * hop_length\n\n        # Generate white noise\n        noise = torch.randn(batch_size, n_samples, device=filter_magnitudes.device)\n        noise_fft = torch.fft.rfft(noise, dim=1)\n        freqs = torch.fft.rfftfreq(n_samples, 1/self.sample_rate).to(filter_magnitudes.device)\n\n        # Upsample filter magnitudes\n        filter_magnitudes_upsampled = F.interpolate(\n            filter_magnitudes.transpose(1, 2), size=n_samples,\n            mode='linear', align_corners=True\n        ).transpose(1, 2)\n\n        # VECTORIZED: Create frequency-domain filter\n        log_freqs = torch.log(freqs + 1e-7).unsqueeze(-1)\n        log_filter_freqs = torch.log(self.filter_freqs + 1e-7).unsqueeze(0)\n        distances = torch.abs(log_freqs - log_filter_freqs)\n        \n        weights = torch.exp(-distances**2 / 0.5)\n        weights = weights / (weights.sum(dim=1, keepdim=True) + 1e-7)\n        \n        filter_response = torch.matmul(\n            filter_magnitudes_upsampled.mean(dim=1),\n            weights.T\n        )\n\n        # Apply filter\n        filtered_fft = noise_fft * filter_response\n        filtered_noise = torch.fft.irfft(filtered_fft, n=n_samples, dim=1)\n\n        return filtered_noise\n\n\nclass DDSPSynthesizer(nn.Module):\n    \"\"\"Neural network that maps features to synthesis parameters.\"\"\"\n\n    def __init__(self, n_harmonics=64, n_filter_banks=64, hidden_size=512, n_mfcc=30):\n        super().__init__()\n        self.n_harmonics = n_harmonics\n        self.n_filter_banks = n_filter_banks\n\n        input_size = 1 + 1 + n_mfcc\n\n        self.gru = nn.GRU(\n            input_size=input_size, hidden_size=hidden_size,\n            num_layers=2, batch_first=True, dropout=0.1\n        )\n\n        # FIX: Remove Softplus, add explicit positive constraint\n        self.harmonic_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_harmonics),\n            nn.ReLU()  # Changed from Softplus to ReLU\n        )\n\n        self.noise_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_filter_banks),\n            nn.Sigmoid()\n        )\n        \n        # FIX: Initialize harmonic output layer with positive bias\n        # Encourage non-zero activations for all harmonics\n        with torch.no_grad():\n            self.harmonic_head[-2].bias.data.fill_(0.1)  # Small positive bias\n\n    def forward(self, f0, loudness, mfcc):\n        x = torch.cat([f0, loudness, mfcc], dim=-1)\n        x, _ = self.gru(x)\n\n        harmonic_amplitudes = self.harmonic_head(x)\n        filter_magnitudes = self.noise_head(x)\n\n        loudness_scale = torch.exp(loudness / 20.0)\n        harmonic_amplitudes = harmonic_amplitudes * loudness_scale\n\n        return harmonic_amplitudes, filter_magnitudes\n\n\nclass DDSPModel(nn.Module):\n    \"\"\"Complete DDSP model.\"\"\"\n\n    def __init__(self, sample_rate=22050, n_harmonics=64, n_filter_banks=64, hidden_size=512):\n        super().__init__()\n        self.sample_rate = sample_rate\n\n        self.synthesizer = DDSPSynthesizer(n_harmonics, n_filter_banks, hidden_size)\n        self.harmonic_osc = HarmonicOscillator(sample_rate, n_harmonics)\n        self.noise_gen = FilteredNoiseGenerator(sample_rate, n_filter_banks)\n\n        self.register_parameter(\n            'harmonic_noise_ratio', nn.Parameter(torch.tensor(0.8))\n        )\n\n    def forward(self, f0, loudness, mfcc):\n        harmonic_amplitudes, filter_magnitudes = self.synthesizer(f0, loudness, mfcc)\n\n        f0_hz = f0.squeeze(-1)\n        harmonic_audio = self.harmonic_osc(f0_hz, harmonic_amplitudes)\n        noise_audio = self.noise_gen(filter_magnitudes)\n\n        ratio = torch.sigmoid(self.harmonic_noise_ratio)\n        audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n\n        return audio, harmonic_audio, noise_audio\n\n\nclass MultiScaleSpectralLoss(nn.Module):\n    \"\"\"Multi-scale spectral loss with optimized FFT scales.\"\"\"\n\n    def __init__(self, fft_sizes=[2048, 512]):\n        super().__init__()\n        self.fft_sizes = fft_sizes\n\n    def forward(self, pred_audio, target_audio):\n        total_loss = 0.0\n\n        for fft_size in self.fft_sizes:\n            pred_stft = torch.stft(\n                pred_audio, n_fft=fft_size, hop_length=fft_size // 4,\n                window=torch.hann_window(fft_size, device=pred_audio.device),\n                return_complex=True\n            )\n            target_stft = torch.stft(\n                target_audio, n_fft=fft_size, hop_length=fft_size // 4,\n                window=torch.hann_window(fft_size, device=target_audio.device),\n                return_complex=True\n            )\n\n            pred_log_mag = torch.log(torch.abs(pred_stft) + 1e-5)\n            target_log_mag = torch.log(torch.abs(target_stft) + 1e-5)\n\n            total_loss += F.l1_loss(pred_log_mag, target_log_mag)\n\n        return total_loss / len(self.fft_sizes)\n\n\nprint(\"‚úÖ DDSP components defined (VECTORIZED + FIXED)\")\nprint(f\"   Fixes: ReLU activation + positive bias initialization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NCA Module\n",
    "\n",
    "Neural Cellular Automaton for organic harmonic evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicNCA(nn.Module):\n",
    "    \"\"\"Neural Cellular Automaton for evolving harmonic masks.\n",
    "    \n",
    "    The NCA learns to grow harmonic activations from a seed (fundamental only)\n",
    "    to full timbre through neighbor interactions and local rules.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_harmonics=64, hidden_channels=16):\n",
    "        super().__init__()\n",
    "        self.n_harmonics = n_harmonics\n",
    "        \n",
    "        # Perception: 1D convolution to sense neighboring harmonics\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            1, hidden_channels, \n",
    "            kernel_size=3, \n",
    "            padding=1, \n",
    "            padding_mode='circular'  # Wrap around for harmonic continuity\n",
    "        )\n",
    "        \n",
    "        # Update rule: compute change based on perception\n",
    "        self.conv2 = nn.Conv1d(hidden_channels, 1, kernel_size=1)\n",
    "        \n",
    "        # Initialize final layer to zero (start with no effect)\n",
    "        self.conv2.weight.data.zero_()\n",
    "        self.conv2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, state, steps):\n",
    "        \"\"\"\n",
    "        Evolve harmonic state over multiple steps.\n",
    "        \n",
    "        Args:\n",
    "            state: (batch, n_harmonics) initial harmonic activations\n",
    "            steps: number of evolution iterations\n",
    "            \n",
    "        Returns:\n",
    "            evolved_state: (batch, n_harmonics) final harmonic activations\n",
    "        \"\"\"\n",
    "        state = state.unsqueeze(1)  # Add channel dimension: (batch, 1, n_harmonics)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            # Perceive neighborhood\n",
    "            x = F.relu(self.conv1(state))\n",
    "            \n",
    "            # Compute update\n",
    "            delta = self.conv2(x)\n",
    "            \n",
    "            # Apply update\n",
    "            state = state + delta\n",
    "            \n",
    "            # Keep in valid range [0, 1]\n",
    "            state = torch.sigmoid(state)\n",
    "        \n",
    "        return state.squeeze(1)  # Remove channel dimension\n",
    "\n",
    "\n",
    "print(\"‚úÖ NCA module defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path, sample_rate=22050):\n",
    "    \"\"\"Extract f0, loudness, and MFCCs from audio.\"\"\"\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)\n",
    "    hop_length = int(sample_rate / 43.066)\n",
    "\n",
    "    # F0\n",
    "    f0_yin = librosa.yin(\n",
    "        audio, fmin=librosa.note_to_hz('C2'),\n",
    "        fmax=librosa.note_to_hz('C7'), sr=sr, hop_length=hop_length\n",
    "    )\n",
    "    f0_yin = np.nan_to_num(f0_yin, nan=0.0)\n",
    "    f0_yin = np.maximum(f0_yin, 0.0)\n",
    "\n",
    "    # Loudness\n",
    "    loudness = librosa.feature.rms(\n",
    "        y=audio, frame_length=2048, hop_length=hop_length\n",
    "    )[0]\n",
    "    loudness_db = librosa.amplitude_to_db(loudness, ref=1.0)\n",
    "\n",
    "    # MFCCs\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=audio, sr=sr, n_mfcc=30, hop_length=hop_length\n",
    "    ).T\n",
    "\n",
    "    min_len = min(len(f0_yin), len(loudness_db), len(mfcc))\n",
    "\n",
    "    return {\n",
    "        'f0': f0_yin[:min_len],\n",
    "        'loudness': loudness_db[:min_len],\n",
    "        'mfcc': mfcc[:min_len],\n",
    "        'audio': audio,\n",
    "        'n_frames': min_len\n",
    "    }\n",
    "\n",
    "\n",
    "# OPTIMIZATION 2: Segment sampling functions\n",
    "def sample_random_segment(features, segment_frames):\n",
    "    \"\"\"Sample random segment from features and audio.\"\"\"\n",
    "    max_start = features['n_frames'] - segment_frames\n",
    "    \n",
    "    if max_start <= 0:\n",
    "        # Audio too short, pad if needed\n",
    "        return features, features['audio']\n",
    "    \n",
    "    start_idx = np.random.randint(0, max_start)\n",
    "    end_idx = start_idx + segment_frames\n",
    "    \n",
    "    # Sample features\n",
    "    segment_features = {\n",
    "        'f0': features['f0'][start_idx:end_idx],\n",
    "        'loudness': features['loudness'][start_idx:end_idx],\n",
    "        'mfcc': features['mfcc'][start_idx:end_idx]\n",
    "    }\n",
    "    \n",
    "    # Sample corresponding audio\n",
    "    audio_start = start_idx * HOP_LENGTH\n",
    "    audio_end = end_idx * HOP_LENGTH\n",
    "    segment_audio = features['audio'][audio_start:audio_end]\n",
    "    \n",
    "    return segment_features, segment_audio\n",
    "\n",
    "\n",
    "# OPTIMIZATION 3: Batch sampling\n",
    "def sample_batch(all_features, batch_size, segment_frames):\n",
    "    \"\"\"Sample batch of random segments from random files.\"\"\"\n",
    "    batch_f0 = []\n",
    "    batch_loudness = []\n",
    "    batch_mfcc = []\n",
    "    batch_audio = []\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        # Pick random file\n",
    "        file_idx = np.random.randint(0, len(all_features))\n",
    "        features = all_features[file_idx]\n",
    "        \n",
    "        # Sample random segment\n",
    "        seg_features, seg_audio = sample_random_segment(features, segment_frames)\n",
    "        \n",
    "        batch_f0.append(seg_features['f0'])\n",
    "        batch_loudness.append(seg_features['loudness'])\n",
    "        batch_mfcc.append(seg_features['mfcc'])\n",
    "        batch_audio.append(seg_audio)\n",
    "    \n",
    "    # Stack into tensors\n",
    "    return {\n",
    "        'f0': torch.tensor(np.stack(batch_f0), dtype=torch.float32).unsqueeze(-1).to(device),\n",
    "        'loudness': torch.tensor(np.stack(batch_loudness), dtype=torch.float32).unsqueeze(-1).to(device),\n",
    "        'mfcc': torch.tensor(np.stack(batch_mfcc), dtype=torch.float32).to(device),\n",
    "        'audio': torch.tensor(np.stack(batch_audio), dtype=torch.float32).to(device)\n",
    "    }\n",
    "\n",
    "\n",
    "# Extract features from all uploaded files\n",
    "print(\"üìä Extracting features from audio files...\")\n",
    "all_features = []\n",
    "\n",
    "for audio_path in audio_paths:\n",
    "    features = extract_features(audio_path)\n",
    "    all_features.append(features)\n",
    "    print(f\"   {audio_path}:\")\n",
    "    print(f\"      F0 range: {features['f0'].min():.1f} - {features['f0'].max():.1f} Hz\")\n",
    "    print(f\"      Frames: {features['n_frames']}\")\n",
    "    print(f\"      Duration: {len(features['audio']) / SAMPLE_RATE:.2f}s\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(all_features)} audio file(s)\")\n",
    "print(f\"   Segment duration: {SEGMENT_DURATION}s ({SEGMENT_FRAMES} frames)\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Listen to first file\n",
    "print(\"\\nüéµ First audio file:\")\n",
    "display(Audio(all_features[0]['audio'], rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train DDSP Model\n",
    "\n",
    "**Optimized training with:**\n",
    "- Reduced FFT scales (2x speedup)\n",
    "- Segment-based training (3-4x speedup)\n",
    "- Batching (2-3x speedup)\n",
    "- Multi-file support\n",
    "\n",
    "**Expected time: 40-60 minutes** (from 8 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create model\nprint(\"üèóÔ∏è  Building DDSP model...\")\nmodel = DDSPModel(\n    sample_rate=SAMPLE_RATE, \n    n_harmonics=N_HARMONICS, \n    n_filter_banks=N_FILTER_BANKS, \n    hidden_size=HIDDEN_SIZE\n).to(device)\n\nn_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"   Parameters: {n_params:,}\")\n\n# Optimizer and loss\noptimizer = optim.Adam(model.parameters(), lr=DDSP_LR)\nspectral_loss_fn = MultiScaleSpectralLoss().to(device)\n\n# Mixed precision training\nscaler = torch.amp.GradScaler('cuda') if USE_AMP and device == 'cuda' else None\n\n# Training loop\nlosses = []\nbest_loss = float('inf')\n\nprint(f\"\\nüéØ Training DDSP for {DDSP_EPOCHS} epochs...\")\nprint(f\"   Optimizations: vectorized synthesis + segment-based ({SEGMENT_DURATION}s)\")\nprint(f\"   + batching (size {BATCH_SIZE}) + reduced FFT scales\")\nprint(f\"   Learning rate: {DDSP_LR} (reduced for stability)\")\nif USE_AMP and device == 'cuda':\n    print(f\"   Mixed precision: ENABLED (2-3x speedup on A100)\")\n\nfor epoch in tqdm(range(DDSP_EPOCHS)):\n    model.train()\n    optimizer.zero_grad()\n\n    # Sample batch of segments from random files\n    batch = sample_batch(all_features, BATCH_SIZE, SEGMENT_FRAMES)\n\n    # Forward pass with mixed precision\n    if USE_AMP and device == 'cuda':\n        with torch.amp.autocast('cuda'):\n            pred_audio, harmonic_audio, noise_audio = model(\n                batch['f0'], \n                batch['loudness'], \n                batch['mfcc']\n            )\n\n            # Get harmonic amplitudes for diversity loss\n            harmonic_amps, _ = model.synthesizer(\n                batch['f0'], \n                batch['loudness'], \n                batch['mfcc']\n            )\n\n            # Compute losses\n            min_len = min(pred_audio.shape[1], batch['audio'].shape[1])\n            pred_audio_trim = pred_audio[:, :min_len]\n            target_audio_trim = batch['audio'][:, :min_len]\n\n            spec_loss = spectral_loss_fn(pred_audio_trim, target_audio_trim)\n            time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n            \n            # FIXED: Direct fundamental ratio penalty\n            fundamental_energy = harmonic_amps[:, :, 0].sum()\n            total_energy = harmonic_amps.sum() + 1e-7\n            fundamental_ratio = fundamental_energy / total_energy\n            \n            # Also encourage upper harmonics directly\n            upper_harmonics_mean = harmonic_amps[:, :, 1:].mean()\n            \n            # Penalize fundamental dominance + encourage upper harmonics\n            harmonic_diversity_loss = fundamental_ratio - 0.1 * upper_harmonics_mean\n            \n            total_loss = 1.0 * spec_loss + 0.1 * time_loss + 1.0 * harmonic_diversity_loss\n\n        # Backward pass with gradient scaling\n        scaler.scale(total_loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        # Standard precision\n        pred_audio, harmonic_audio, noise_audio = model(\n            batch['f0'], \n            batch['loudness'], \n            batch['mfcc']\n        )\n\n        harmonic_amps, _ = model.synthesizer(\n            batch['f0'], \n            batch['loudness'], \n            batch['mfcc']\n        )\n\n        # Compute losses\n        min_len = min(pred_audio.shape[1], batch['audio'].shape[1])\n        pred_audio_trim = pred_audio[:, :min_len]\n        target_audio_trim = batch['audio'][:, :min_len]\n\n        spec_loss = spectral_loss_fn(pred_audio_trim, target_audio_trim)\n        time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n        \n        # Fixed harmonic diversity loss\n        fundamental_energy = harmonic_amps[:, :, 0].sum()\n        total_energy = harmonic_amps.sum() + 1e-7\n        fundamental_ratio = fundamental_energy / total_energy\n        upper_harmonics_mean = harmonic_amps[:, :, 1:].mean()\n        harmonic_diversity_loss = fundamental_ratio - 0.1 * upper_harmonics_mean\n        \n        total_loss = 1.0 * spec_loss + 0.1 * time_loss + 1.0 * harmonic_diversity_loss\n\n        # Backward pass\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n    # Track loss\n    loss_value = total_loss.item()\n    losses.append(loss_value)\n\n    if loss_value < best_loss:\n        best_loss = loss_value\n\n    if (epoch + 1) % 200 == 0:\n        print(f\"\\nEpoch {epoch+1}: Loss={loss_value:.6f}, Best={best_loss:.6f}\")\n\nprint(f\"\\n‚úÖ DDSP training complete! Best loss: {best_loss:.6f}\")\n\n# Plot training curve\nplt.figure(figsize=(10, 5))\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('DDSP Training Loss')\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test DDSP Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random segment\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_batch = sample_batch(all_features, 1, SEGMENT_FRAMES)\n",
    "    pred_audio, harmonic_audio, noise_audio = model(\n",
    "        test_batch['f0'],\n",
    "        test_batch['loudness'],\n",
    "        test_batch['mfcc']\n",
    "    )\n",
    "\n",
    "pred_audio_np = pred_audio.squeeze().cpu().numpy()\n",
    "target_audio_np = test_batch['audio'].squeeze().cpu().numpy()\n",
    "harmonic_audio_np = harmonic_audio.squeeze().cpu().numpy()\n",
    "noise_audio_np = noise_audio.squeeze().cpu().numpy()\n",
    "\n",
    "print(\"üéµ Target audio:\")\n",
    "display(Audio(target_audio_np, rate=SAMPLE_RATE))\n",
    "\n",
    "print(\"\\nüéµ DDSP reconstructed audio:\")\n",
    "display(Audio(pred_audio_np, rate=SAMPLE_RATE))\n",
    "\n",
    "print(\"\\nüéµ Harmonic component only:\")\n",
    "display(Audio(harmonic_audio_np, rate=SAMPLE_RATE))\n",
    "\n",
    "print(\"\\nüéµ Noise component only:\")\n",
    "display(Audio(noise_audio_np, rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train NCA Controller\n",
    "\n",
    "Train the NCA to generate organic harmonic evolution patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"üß¨ Training NCA controller...\")\n\n# Create NCA\nnca = HarmonicNCA(n_harmonics=N_HARMONICS, hidden_channels=16).to(device)\nnca_optimizer = optim.Adam(nca.parameters(), lr=NCA_LR)\n\n# Mixed precision scaler for NCA\nnca_scaler = torch.amp.GradScaler('cuda') if USE_AMP and device == 'cuda' else None\n\nnca_losses = []\nbest_nca_loss = float('inf')\n\n# Freeze DDSP model (only train NCA)\nmodel.eval()\nfor param in model.parameters():\n    param.requires_grad = False\n\nprint(f\"   NCA parameters: {sum(p.numel() for p in nca.parameters()):,}\")\nprint(f\"   Evolution steps: {NCA_STEPS}\")\n\nfor epoch in tqdm(range(NCA_EPOCHS)):\n    nca.train()\n    nca_optimizer.zero_grad()\n\n    # Sample batch\n    batch = sample_batch(all_features, BATCH_SIZE, SEGMENT_FRAMES)\n\n    # Initialize seed (just fundamental harmonic)\n    seed = torch.zeros(BATCH_SIZE, N_HARMONICS, device=device)\n    seed[:, 0] = 1.0  # Fundamental active\n\n    if USE_AMP and device == 'cuda':\n        with torch.amp.autocast('cuda'):\n            # Evolve with NCA\n            evolved_mask = nca(seed, NCA_STEPS)\n\n            # Get DDSP harmonic amplitudes (DDSP frozen, but gradients flow through)\n            with torch.no_grad():\n                harmonic_amps, filter_mags = model.synthesizer(\n                    batch['f0'],\n                    batch['loudness'],\n                    batch['mfcc']\n                )\n\n            # Apply NCA mask to harmonic amplitudes\n            # evolved_mask: (batch, n_harmonics)\n            # harmonic_amps: (batch, n_frames, n_harmonics)\n            masked_amps = harmonic_amps * evolved_mask.unsqueeze(1)\n\n            # Synthesize audio (GRADIENTS FLOW TO NCA)\n            f0_hz = batch['f0'].squeeze(-1)\n            harmonic_audio = model.harmonic_osc(f0_hz, masked_amps)\n            \n            with torch.no_grad():\n                noise_audio = model.noise_gen(filter_mags)\n                ratio = torch.sigmoid(model.harmonic_noise_ratio)\n            \n            pred_audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n\n            # Loss: compare to target audio\n            min_len = min(pred_audio.shape[1], batch['audio'].shape[1])\n            pred_audio_trim = pred_audio[:, :min_len]\n            target_audio_trim = batch['audio'][:, :min_len]\n\n            spec_loss = spectral_loss_fn(pred_audio_trim, target_audio_trim)\n            time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n            total_loss = 1.0 * spec_loss + 0.1 * time_loss\n\n        # Backward pass with gradient scaling (only NCA parameters updated)\n        nca_scaler.scale(total_loss).backward()\n        nca_scaler.unscale_(nca_optimizer)\n        torch.nn.utils.clip_grad_norm_(nca.parameters(), max_norm=1.0)\n        nca_scaler.step(nca_optimizer)\n        nca_scaler.update()\n    else:\n        # Standard precision\n        # Evolve with NCA\n        evolved_mask = nca(seed, NCA_STEPS)\n\n        # Get DDSP harmonic amplitudes\n        with torch.no_grad():\n            harmonic_amps, filter_mags = model.synthesizer(\n                batch['f0'],\n                batch['loudness'],\n                batch['mfcc']\n            )\n\n        # Apply NCA mask\n        masked_amps = harmonic_amps * evolved_mask.unsqueeze(1)\n\n        # Synthesize audio (GRADIENTS FLOW TO NCA)\n        f0_hz = batch['f0'].squeeze(-1)\n        harmonic_audio = model.harmonic_osc(f0_hz, masked_amps)\n        \n        with torch.no_grad():\n            noise_audio = model.noise_gen(filter_mags)\n            ratio = torch.sigmoid(model.harmonic_noise_ratio)\n        \n        pred_audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n\n        # Loss\n        min_len = min(pred_audio.shape[1], batch['audio'].shape[1])\n        pred_audio_trim = pred_audio[:, :min_len]\n        target_audio_trim = batch['audio'][:, :min_len]\n\n        spec_loss = spectral_loss_fn(pred_audio_trim, target_audio_trim)\n        time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n        total_loss = 1.0 * spec_loss + 0.1 * time_loss\n\n        # Backward pass (only NCA parameters updated)\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(nca.parameters(), max_norm=1.0)\n        nca_optimizer.step()\n\n    # Track loss\n    loss_value = total_loss.item()\n    nca_losses.append(loss_value)\n\n    if loss_value < best_nca_loss:\n        best_nca_loss = loss_value\n\n    if (epoch + 1) % 100 == 0:\n        print(f\"\\nNCA Epoch {epoch+1}: Loss={loss_value:.6f}, Best={best_nca_loss:.6f}\")\n\nprint(f\"\\n‚úÖ NCA training complete! Best loss: {best_nca_loss:.6f}\")\n\n# Plot NCA training curve\nplt.figure(figsize=(10, 5))\nplt.plot(nca_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('NCA Training Loss')\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Re-enable DDSP gradients\nfor param in model.parameters():\n    param.requires_grad = True"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate NCA-Controlled Growing Timbre\n",
    "\n",
    "Use the trained NCA to generate organic harmonic evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_nca_stage(model, nca, features, nca_step, duration=0.5, device='cpu'):\n",
    "    \"\"\"Synthesize one stage with NCA-controlled harmonic evolution.\"\"\"\n",
    "    hop_length = 512\n",
    "    n_frames = int((duration * SAMPLE_RATE) / hop_length)\n",
    "\n",
    "    # Get mean features for synthesis\n",
    "    f0_mean = features['f0'][features['f0'] > 0].mean()\n",
    "    if np.isnan(f0_mean):\n",
    "        f0_mean = 220.0\n",
    "\n",
    "    f0_tensor = torch.ones(1, n_frames, 1, device=device) * f0_mean\n",
    "    loudness_tensor = torch.ones(1, n_frames, 1, device=device) * features['loudness'].mean()\n",
    "    mfcc_mean = torch.tensor(features['mfcc'].mean(axis=0), dtype=torch.float32, device=device)\n",
    "    mfcc_tensor = mfcc_mean.unsqueeze(0).unsqueeze(0).expand(1, n_frames, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize NCA seed\n",
    "        seed = torch.zeros(1, N_HARMONICS, device=device)\n",
    "        seed[0, 0] = 1.0  # Fundamental\n",
    "\n",
    "        # Evolve NCA to current step\n",
    "        nca_mask = nca(seed, nca_step)\n",
    "\n",
    "        # Get DDSP synthesis parameters\n",
    "        harmonic_amplitudes, filter_magnitudes = model.synthesizer(\n",
    "            f0_tensor, loudness_tensor, mfcc_tensor\n",
    "        )\n",
    "\n",
    "        # Apply NCA mask\n",
    "        masked_harmonics = harmonic_amplitudes * nca_mask.unsqueeze(1)\n",
    "\n",
    "        # Generate audio\n",
    "        f0_hz = f0_tensor.squeeze(-1)\n",
    "        harmonic_audio = model.harmonic_osc(f0_hz, masked_harmonics)\n",
    "        noise_audio = model.noise_gen(filter_magnitudes)\n",
    "\n",
    "        ratio = torch.sigmoid(model.harmonic_noise_ratio)\n",
    "        pred_audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n",
    "\n",
    "    audio = pred_audio.squeeze().cpu().numpy()\n",
    "    target_samples = int(duration * SAMPLE_RATE)\n",
    "    if len(audio) < target_samples:\n",
    "        audio = np.pad(audio, (0, target_samples - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_samples]\n",
    "\n",
    "    # Normalize\n",
    "    if np.abs(audio).max() > 0:\n",
    "        audio = audio / np.abs(audio).max() * 0.8\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "# Generate NCA-controlled growing stages\n",
    "print(\"üéµ Generating NCA-controlled growing timbre...\")\n",
    "STAGE_DURATION = 0.5\n",
    "SILENCE_DURATION = 0.2\n",
    "N_STAGES = 32  # Number of discrete stages\n",
    "\n",
    "model.eval()\n",
    "nca.eval()\n",
    "\n",
    "audio_segments = []\n",
    "silence = np.zeros(int(SILENCE_DURATION * SAMPLE_RATE))\n",
    "\n",
    "# Generate stages with progressive NCA evolution\n",
    "for stage in tqdm(range(N_STAGES)):\n",
    "    # NCA step increases with stage (0, 1, 2, ..., 31)\n",
    "    stage_audio = synthesize_nca_stage(\n",
    "        model, nca, all_features[0], \n",
    "        nca_step=stage,  # Progressive evolution\n",
    "        duration=STAGE_DURATION, \n",
    "        device=device\n",
    "    )\n",
    "    audio_segments.append(stage_audio)\n",
    "    \n",
    "    if stage < N_STAGES - 1:\n",
    "        audio_segments.append(silence)\n",
    "\n",
    "full_audio = np.concatenate(audio_segments)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {N_STAGES} NCA-controlled stages\")\n",
    "print(f\"   Total duration: {len(full_audio)/SAMPLE_RATE:.2f}s\")\n",
    "print(f\"   Evolution: Organic NCA-controlled harmonic growth\")\n",
    "\n",
    "# Listen\n",
    "print(\"\\nüéß NCA-controlled growing timbre:\")\n",
    "display(Audio(full_audio, rate=SAMPLE_RATE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize NCA Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how NCA evolves harmonic masks\n",
    "print(\"üìä Visualizing NCA evolution...\")\n",
    "\n",
    "nca.eval()\n",
    "with torch.no_grad():\n",
    "    seed = torch.zeros(1, N_HARMONICS, device=device)\n",
    "    seed[0, 0] = 1.0\n",
    "    \n",
    "    # Capture evolution at multiple steps\n",
    "    evolution_steps = [0, 4, 8, 16, 24, 32]\n",
    "    evolution_masks = []\n",
    "    \n",
    "    for step in evolution_steps:\n",
    "        mask = nca(seed.clone(), step)\n",
    "        evolution_masks.append(mask.cpu().numpy()[0])\n",
    "\n",
    "# Plot evolution\n",
    "fig, axes = plt.subplots(len(evolution_steps), 1, figsize=(12, 10), sharex=True)\n",
    "fig.suptitle('NCA Harmonic Evolution', fontsize=16)\n",
    "\n",
    "for idx, (step, mask) in enumerate(zip(evolution_steps, evolution_masks)):\n",
    "    axes[idx].bar(range(N_HARMONICS), mask, color='steelblue', alpha=0.7)\n",
    "    axes[idx].set_ylabel(f'Step {step}')\n",
    "    axes[idx].set_ylim(0, 1)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Harmonic Number')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ NCA evolves harmonics organically through cellular automaton dynamics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files\n",
    "sf.write('ddsp_nca_grown_timbre.wav', full_audio, SAMPLE_RATE)\n",
    "\n",
    "# Save models\n",
    "torch.save({\n",
    "    'ddsp_state_dict': model.state_dict(),\n",
    "    'nca_state_dict': nca.state_dict(),\n",
    "    'config': {\n",
    "        'n_harmonics': N_HARMONICS,\n",
    "        'n_filter_banks': N_FILTER_BANKS,\n",
    "        'hidden_size': HIDDEN_SIZE,\n",
    "        'sample_rate': SAMPLE_RATE,\n",
    "        'nca_steps': NCA_STEPS\n",
    "    }\n",
    "}, 'ddsp_nca_model.pt')\n",
    "\n",
    "# Download in Colab\n",
    "from google.colab import files\n",
    "files.download('ddsp_nca_grown_timbre.wav')\n",
    "files.download('ddsp_nca_model.pt')\n",
    "\n",
    "print(\"‚úÖ Files ready for download!\")\n",
    "print(\"\\nüìà Performance summary:\")\n",
    "print(f\"   DDSP training: Optimized with segment-based + batching + reduced FFT\")\n",
    "print(f\"   NCA training: Lightweight cellular automaton controller\")\n",
    "print(f\"   Result: Organic harmonic evolution with high-quality DDSP synthesis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}