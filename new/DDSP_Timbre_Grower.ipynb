{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP Timbre Grower - Complete Implementation\n",
    "\n",
    "This notebook trains a lightweight DDSP model and generates discrete growing stages.\n",
    "\n",
    "**Runtime**: Enable GPU in Colab for 5-10x speedup!\n",
    "\n",
    "**Workflow**:\n",
    "1. Upload target audio (violin.wav)\n",
    "2. Install dependencies\n",
    "3. Define DDSP components\n",
    "4. Train model (~5-10 min on GPU)\n",
    "5. Generate discrete growing stages\n",
    "6. Download results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch librosa soundfile matplotlib scipy tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal as scipy_signal\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Check for GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Target Audio\n",
    "\n",
    "Upload your `violin.wav` file (or any mono audio file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded filename\n",
    "audio_path = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded: {audio_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DDSP Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicOscillator(nn.Module):\n",
    "    \"\"\"Differentiable harmonic oscillator for additive synthesis.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=22050, n_harmonics=64):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_harmonics = n_harmonics\n",
    "    \n",
    "    def forward(self, f0_hz, harmonic_amplitudes):\n",
    "        batch_size, n_frames = f0_hz.shape\n",
    "        hop_length = 512\n",
    "        n_samples = n_frames * hop_length\n",
    "        \n",
    "        # Upsample f0 and amplitudes\n",
    "        f0_upsampled = F.interpolate(\n",
    "            f0_hz.unsqueeze(1), size=n_samples, mode='linear', align_corners=True\n",
    "        ).squeeze(1)\n",
    "        \n",
    "        harmonic_amplitudes_upsampled = F.interpolate(\n",
    "            harmonic_amplitudes.transpose(1, 2), size=n_samples, \n",
    "            mode='linear', align_corners=True\n",
    "        ).transpose(1, 2)\n",
    "        \n",
    "        # Compute phase\n",
    "        phase = 2 * np.pi * torch.cumsum(f0_upsampled / self.sample_rate, dim=1)\n",
    "        \n",
    "        # Generate harmonics\n",
    "        audio = torch.zeros(batch_size, n_samples, device=f0_hz.device)\n",
    "        for h in range(self.n_harmonics):\n",
    "            harmonic_phase = phase * (h + 1)\n",
    "            harmonic_signal = torch.sin(harmonic_phase)\n",
    "            harmonic_signal = harmonic_signal * harmonic_amplitudes_upsampled[:, :, h]\n",
    "            audio += harmonic_signal\n",
    "        \n",
    "        return audio\n",
    "\n",
    "\n",
    "class FilteredNoiseGenerator(nn.Module):\n",
    "    \"\"\"Differentiable filtered noise generator.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=22050, n_filter_banks=64):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_filter_banks = n_filter_banks\n",
    "        \n",
    "        self.register_buffer(\n",
    "            'filter_freqs',\n",
    "            torch.logspace(np.log10(20), np.log10(sample_rate / 2), n_filter_banks)\n",
    "        )\n",
    "    \n",
    "    def forward(self, filter_magnitudes):\n",
    "        batch_size, n_frames, _ = filter_magnitudes.shape\n",
    "        hop_length = 512\n",
    "        n_samples = n_frames * hop_length\n",
    "        \n",
    "        # Generate white noise\n",
    "        noise = torch.randn(batch_size, n_samples, device=filter_magnitudes.device)\n",
    "        noise_fft = torch.fft.rfft(noise, dim=1)\n",
    "        freqs = torch.fft.rfftfreq(n_samples, 1/self.sample_rate).to(filter_magnitudes.device)\n",
    "        \n",
    "        # Upsample filter magnitudes\n",
    "        filter_magnitudes_upsampled = F.interpolate(\n",
    "            filter_magnitudes.transpose(1, 2), size=n_samples,\n",
    "            mode='linear', align_corners=True\n",
    "        ).transpose(1, 2)\n",
    "        \n",
    "        # Create frequency-domain filter\n",
    "        filter_response = torch.zeros(batch_size, len(freqs), device=filter_magnitudes.device)\n",
    "        for i, freq in enumerate(freqs):\n",
    "            distances = torch.abs(torch.log(self.filter_freqs + 1e-7) - np.log(freq + 1e-7))\n",
    "            weights = torch.exp(-distances**2 / 0.5)\n",
    "            weights = weights / (weights.sum() + 1e-7)\n",
    "            filter_value = (filter_magnitudes_upsampled.mean(dim=1) * weights).sum(dim=1)\n",
    "            filter_response[:, i] = filter_value\n",
    "        \n",
    "        # Apply filter\n",
    "        filtered_fft = noise_fft * filter_response\n",
    "        filtered_noise = torch.fft.irfft(filtered_fft, n=n_samples, dim=1)\n",
    "        \n",
    "        return filtered_noise\n",
    "\n",
    "\n",
    "class DDSPSynthesizer(nn.Module):\n",
    "    \"\"\"Neural network that maps features to synthesis parameters.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_harmonics=64, n_filter_banks=64, hidden_size=512, n_mfcc=30):\n",
    "        super().__init__()\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_filter_banks = n_filter_banks\n",
    "        \n",
    "        input_size = 1 + 1 + n_mfcc  # f0 + loudness + MFCCs\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size, hidden_size=hidden_size,\n",
    "            num_layers=2, batch_first=True, dropout=0.1\n",
    "        )\n",
    "        \n",
    "        self.harmonic_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_harmonics),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.noise_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_filter_banks),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, f0, loudness, mfcc):\n",
    "        x = torch.cat([f0, loudness, mfcc], dim=-1)\n",
    "        x, _ = self.gru(x)\n",
    "        \n",
    "        harmonic_amplitudes = self.harmonic_head(x)\n",
    "        filter_magnitudes = self.noise_head(x)\n",
    "        \n",
    "        loudness_scale = torch.exp(loudness / 20.0)\n",
    "        harmonic_amplitudes = harmonic_amplitudes * loudness_scale\n",
    "        \n",
    "        return harmonic_amplitudes, filter_magnitudes\n",
    "\n",
    "\n",
    "class DDSPModel(nn.Module):\n",
    "    \"\"\"Complete DDSP model.\"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=22050, n_harmonics=64, n_filter_banks=64, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        self.synthesizer = DDSPSynthesizer(n_harmonics, n_filter_banks, hidden_size)\n",
    "        self.harmonic_osc = HarmonicOscillator(sample_rate, n_harmonics)\n",
    "        self.noise_gen = FilteredNoiseGenerator(sample_rate, n_filter_banks)\n",
    "        \n",
    "        self.register_parameter(\n",
    "            'harmonic_noise_ratio', nn.Parameter(torch.tensor(0.8))\n",
    "        )\n",
    "    \n",
    "    def forward(self, f0, loudness, mfcc):\n",
    "        harmonic_amplitudes, filter_magnitudes = self.synthesizer(f0, loudness, mfcc)\n",
    "        \n",
    "        f0_hz = f0.squeeze(-1)\n",
    "        harmonic_audio = self.harmonic_osc(f0_hz, harmonic_amplitudes)\n",
    "        noise_audio = self.noise_gen(filter_magnitudes)\n",
    "        \n",
    "        ratio = torch.sigmoid(self.harmonic_noise_ratio)\n",
    "        audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n",
    "        \n",
    "        return audio, harmonic_audio, noise_audio\n",
    "\n",
    "\n",
    "class MultiScaleSpectralLoss(nn.Module):\n",
    "    \"\"\"Multi-scale spectral loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, fft_sizes=[2048, 1024, 512, 256]):\n",
    "        super().__init__()\n",
    "        self.fft_sizes = fft_sizes\n",
    "    \n",
    "    def forward(self, pred_audio, target_audio):\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for fft_size in self.fft_sizes:\n",
    "            pred_stft = torch.stft(\n",
    "                pred_audio, n_fft=fft_size, hop_length=fft_size // 4,\n",
    "                window=torch.hann_window(fft_size, device=pred_audio.device),\n",
    "                return_complex=True\n",
    "            )\n",
    "            target_stft = torch.stft(\n",
    "                target_audio, n_fft=fft_size, hop_length=fft_size // 4,\n",
    "                window=torch.hann_window(fft_size, device=target_audio.device),\n",
    "                return_complex=True\n",
    "            )\n",
    "            \n",
    "            pred_log_mag = torch.log(torch.abs(pred_stft) + 1e-5)\n",
    "            target_log_mag = torch.log(torch.abs(target_stft) + 1e-5)\n",
    "            \n",
    "            total_loss += F.l1_loss(pred_log_mag, target_log_mag)\n",
    "        \n",
    "        return total_loss / len(self.fft_sizes)\n",
    "\n",
    "\n",
    "print(\"‚úÖ DDSP components defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path, sample_rate=22050):\n",
    "    \"\"\"Extract f0, loudness, and MFCCs from audio.\"\"\"\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)\n",
    "    hop_length = int(sample_rate / 43.066)\n",
    "    \n",
    "    # F0\n",
    "    f0_yin = librosa.yin(\n",
    "        audio, fmin=librosa.note_to_hz('C2'),\n",
    "        fmax=librosa.note_to_hz('C7'), sr=sr, hop_length=hop_length\n",
    "    )\n",
    "    f0_yin = np.nan_to_num(f0_yin, nan=0.0)\n",
    "    f0_yin = np.maximum(f0_yin, 0.0)\n",
    "    \n",
    "    # Loudness\n",
    "    loudness = librosa.feature.rms(\n",
    "        y=audio, frame_length=2048, hop_length=hop_length\n",
    "    )[0]\n",
    "    loudness_db = librosa.amplitude_to_db(loudness, ref=1.0)\n",
    "    \n",
    "    # MFCCs\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=audio, sr=sr, n_mfcc=30, hop_length=hop_length\n",
    "    ).T\n",
    "    \n",
    "    min_len = min(len(f0_yin), len(loudness_db), len(mfcc))\n",
    "    \n",
    "    return {\n",
    "        'f0': f0_yin[:min_len],\n",
    "        'loudness': loudness_db[:min_len],\n",
    "        'mfcc': mfcc[:min_len],\n",
    "        'audio': audio,\n",
    "        'n_frames': min_len\n",
    "    }\n",
    "\n",
    "\n",
    "# Extract features\n",
    "print(\"üìä Extracting features...\")\n",
    "features = extract_features(audio_path)\n",
    "print(f\"   F0 range: {features['f0'].min():.1f} - {features['f0'].max():.1f} Hz\")\n",
    "print(f\"   Frames: {features['n_frames']}\")\n",
    "print(f\"   Duration: {len(features['audio']) / 22050:.2f}s\")\n",
    "\n",
    "# Listen to original\n",
    "print(\"\\nüéµ Original audio:\")\n",
    "display(Audio(features['audio'], rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train DDSP Model\n",
    "\n",
    "This will take ~5-10 minutes on GPU, ~30 minutes on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tensors\n",
    "f0 = torch.tensor(features['f0'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "loudness = torch.tensor(features['loudness'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "mfcc = torch.tensor(features['mfcc'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "target_audio = torch.tensor(features['audio'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "# Create model\n",
    "print(\"üèóÔ∏è  Building model...\")\n",
    "model = DDSPModel(sample_rate=22050, n_harmonics=64, n_filter_banks=64, hidden_size=512).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Parameters: {n_params:,}\")\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "spectral_loss_fn = MultiScaleSpectralLoss().to(device)\n",
    "\n",
    "# Training loop\n",
    "N_EPOCHS = 1000\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(f\"\\nüéØ Training for {N_EPOCHS} epochs...\")\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred_audio, harmonic_audio, noise_audio = model(f0, loudness, mfcc)\n",
    "    \n",
    "    min_len = min(pred_audio.shape[1], target_audio.shape[1])\n",
    "    pred_audio_trim = pred_audio[:, :min_len]\n",
    "    target_audio_trim = target_audio[:, :min_len]\n",
    "    \n",
    "    spec_loss = spectral_loss_fn(pred_audio_trim, target_audio_trim)\n",
    "    time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n",
    "    total_loss = 1.0 * spec_loss + 0.1 * time_loss\n",
    "    \n",
    "    total_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_value = total_loss.item()\n",
    "    losses.append(loss_value)\n",
    "    \n",
    "    if loss_value < best_loss:\n",
    "        best_loss = loss_value\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"\\nEpoch {epoch+1}: Loss={loss_value:.6f}, Best={best_loss:.6f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Best loss: {best_loss:.6f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('DDSP Training Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_audio, harmonic_audio, noise_audio = model(f0, loudness, mfcc)\n",
    "\n",
    "pred_audio_np = pred_audio.squeeze().cpu().numpy()\n",
    "harmonic_audio_np = harmonic_audio.squeeze().cpu().numpy()\n",
    "noise_audio_np = noise_audio.squeeze().cpu().numpy()\n",
    "\n",
    "print(\"üéµ Reconstructed audio:\")\n",
    "display(Audio(pred_audio_np, rate=22050))\n",
    "\n",
    "print(\"\\nüéµ Harmonic component only:\")\n",
    "display(Audio(harmonic_audio_np, rate=22050))\n",
    "\n",
    "print(\"\\nüéµ Noise component only:\")\n",
    "display(Audio(noise_audio_np, rate=22050))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Discrete Growing Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ADSREnvelope:\n    def __init__(self, sr=22050):\n        self.sr = sr\n    \n    def generate(self, duration, attack=0.05, decay=0.1, sustain_level=0.7, release=0.2):\n        n_samples = int(duration * self.sr)\n        attack_samples = int(attack * self.sr)\n        decay_samples = int(decay * self.sr)\n        release_samples = int(release * self.sr)\n        sustain_samples = n_samples - attack_samples - decay_samples - release_samples\n        sustain_samples = max(0, sustain_samples)\n        \n        envelope = []\n        if attack_samples > 0:\n            envelope.extend(np.linspace(0, 1, attack_samples))\n        if decay_samples > 0:\n            envelope.extend(np.linspace(1, sustain_level, decay_samples))\n        if sustain_samples > 0:\n            envelope.extend(np.ones(sustain_samples) * sustain_level)\n        if release_samples > 0:\n            envelope.extend(np.linspace(sustain_level, 0, release_samples))\n        \n        envelope = np.array(envelope)\n        if len(envelope) < n_samples:\n            envelope = np.pad(envelope, (0, n_samples - len(envelope)))\n        elif len(envelope) > n_samples:\n            envelope = envelope[:n_samples]\n        return envelope\n\n\ndef synthesize_stage(model, features, active_harmonics, duration=0.5, device='cpu'):\n    \"\"\"Synthesize one stage with progressive harmonic activation.\"\"\"\n    hop_length = 512\n    n_frames = int((duration * 22050) / hop_length)\n    \n    f0_mean = features['f0'][features['f0'] > 0].mean()\n    if np.isnan(f0_mean):\n        f0_mean = 220.0\n    \n    f0_tensor = torch.ones(1, n_frames, 1, device=device) * f0_mean\n    loudness_tensor = torch.ones(1, n_frames, 1, device=device) * features['loudness'].mean()\n    mfcc_mean = torch.tensor(features['mfcc'].mean(axis=0), dtype=torch.float32, device=device)\n    mfcc_tensor = mfcc_mean.unsqueeze(0).unsqueeze(0).expand(1, n_frames, -1)\n    \n    with torch.no_grad():\n        # Get synthesis parameters from model\n        harmonic_amplitudes, filter_magnitudes = model.synthesizer(f0_tensor, loudness_tensor, mfcc_tensor)\n        \n        # Apply harmonic mask to progressively add harmonics\n        harmonic_mask = torch.tensor(active_harmonics, dtype=torch.float32, device=device)\n        harmonic_mask = harmonic_mask.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, n_harmonics]\n        masked_harmonics = harmonic_amplitudes * harmonic_mask\n        \n        # Generate audio with masked harmonics\n        f0_hz = f0_tensor.squeeze(-1)\n        harmonic_audio = model.harmonic_osc(f0_hz, masked_harmonics)\n        noise_audio = model.noise_gen(filter_magnitudes)\n        \n        ratio = torch.sigmoid(model.harmonic_noise_ratio)\n        pred_audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n    \n    audio = pred_audio.squeeze().cpu().numpy()\n    target_samples = int(duration * 22050)\n    if len(audio) < target_samples:\n        audio = np.pad(audio, (0, target_samples - len(audio)))\n    else:\n        audio = audio[:target_samples]\n    \n    # Apply ADSR envelope\n    envelope_gen = ADSREnvelope(sr=22050)\n    envelope = envelope_gen.generate(duration)\n    audio = audio * envelope\n    \n    # Normalize\n    if np.abs(audio).max() > 0:\n        audio = audio / np.abs(audio).max() * 0.8\n    \n    return audio\n\n\ndef generate_stages(strategy='linear', n_harmonics=64):\n    \"\"\"Generate stage masks.\"\"\"\n    stages = []\n    if strategy == 'linear':\n        for n in range(1, n_harmonics + 1):\n            stage = np.zeros(n_harmonics)\n            stage[:n] = 1.0\n            stages.append(stage)\n    return stages\n\n\n# Generate discrete growing stages\nprint(\"üéµ Generating discrete growing stages...\")\nSTAGE_DURATION = 0.5\nSILENCE_DURATION = 0.2\n\nstages = generate_stages('linear', 64)\naudio_segments = []\nsilence = np.zeros(int(SILENCE_DURATION * 22050))\n\nfor i, stage in enumerate(tqdm(stages)):\n    stage_audio = synthesize_stage(model, features, stage, STAGE_DURATION, device)\n    audio_segments.append(stage_audio)\n    if i < len(stages) - 1:\n        audio_segments.append(silence)\n\nfull_audio = np.concatenate(audio_segments)\n\nprint(f\"\\n‚úÖ Generated {len(stages)} stages\")\nprint(f\"   Total duration: {len(full_audio)/22050:.2f}s\")\n\n# Listen\nprint(\"\\nüéß Discrete growing stages with DDSP:\")\ndisplay(Audio(full_audio, rate=22050))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files\n",
    "sf.write('ddsp_reconstructed.wav', pred_audio_np, 22050)\n",
    "sf.write('ddsp_grown_timbre.wav', full_audio, 22050)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'features': features,\n",
    "}, 'ddsp_model.pt')\n",
    "\n",
    "# Download in Colab\n",
    "from google.colab import files\n",
    "files.download('ddsp_reconstructed.wav')\n",
    "files.download('ddsp_grown_timbre.wav')\n",
    "files.download('ddsp_model.pt')\n",
    "\n",
    "print(\"‚úÖ Files ready for download!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}