{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyzpN5tvNebU"
   },
   "source": [
    "# DDSP Timbre Grower with NCA Mutation - Complete Implementation\n",
    "\n",
    "This notebook trains a lightweight DDSP model and then uses a Neural Cellular Automaton (NCA) to creatively evolve the sound's timbre.\n",
    "\n",
    "**Runtime**: Enable GPU in Colab for 5-10x speedup!\n",
    "\n",
    "**Workflow**:\n",
    "1. Upload target audio (violin.wav)\n",
    "2. Install dependencies\n",
    "3. Define DDSP and NCA components\n",
    "4. Train DDSP model (~5-10 min on GPU)\n",
    "5. Use the trained DDSP to provide a \"seed\" for the NCA\n",
    "6. Generate new audio by letting the NCA evolve the timbre\n",
    "7. Download results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWERy8lbNebU"
   },
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzm_7mFFNebV"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install librosa soundfile matplotlib numpy\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY_W9xHhNebV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3iU-fD2NebV"
   },
   "source": [
    "## 2. Upload Audio & Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-x0X3VGNebV"
   },
   "outputs": [],
   "source": [
    "uploaded = files.upload()\n",
    "if not uploaded:\n",
    "    print(\"No file uploaded. Please upload an audio file.\")\n",
    "else:\n",
    "    file_name = next(iter(uploaded))\n",
    "    audio, sr = librosa.load(io.BytesIO(uploaded[file_name]), sr=22050)\n",
    "    print(f\"Loaded '{file_name}' at {sr} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r8E9Z4uNebW"
   },
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "def extract_features(audio, sr):\n",
    "    f0 = librosa.yin(audio, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=30)\n",
    "\n",
    "    # Ensure all features have the same length\n",
    "    min_len = min(len(f0), len(rms), mfcc.shape[1])\n",
    "    f0 = f0[:min_len]\n",
    "    rms = rms[:min_len]\n",
    "    mfcc = mfcc[:, :min_len]\n",
    "\n",
    "    # Convert to tensors\n",
    "    f0 = torch.tensor(f0, dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "    loudness = torch.tensor(rms, dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "    mfcc = torch.tensor(mfcc.T, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    return {'f0': f0, 'loudness': loudness, 'mfcc': mfcc}\n",
    "\n",
    "features = extract_features(audio, sr)\n",
    "audio_tensor = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S8m5p3wNebW"
   },
   "source": [
    "## 3. DDSP + NCA Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-j1jI77NebW"
   },
   "outputs": [],
   "source": [
    "class HarmonicOscillator(nn.Module):\n",
    "    def __init__(self, n_harmonics, sr):\n",
    "        super().__init__()\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.sr = sr\n",
    "\n",
    "    def forward(self, f0, amplitudes):\n",
    "        # f0: (batch, time, 1), amplitudes: (batch, time, n_harmonics + 1)\n",
    "        batch_size, time_steps, _ = f0.shape\n",
    "        \n",
    "        # Generate harmonic frequencies\n",
    "        harmonic_multipliers = torch.arange(1, self.n_harmonics + 2, device=f0.device).float()\n",
    "        harmonic_frequencies = f0 * harmonic_multipliers\n",
    "\n",
    "        # Angular frequency\n",
    "        omegas = 2 * np.pi * harmonic_frequencies / self.sr\n",
    "\n",
    "        # Phases\n",
    "        phases = torch.cumsum(omegas, dim=1)\n",
    "\n",
    "        # Generate sinusoids\n",
    "        sinusoids = torch.sin(phases)\n",
    "\n",
    "        # Apply amplitudes\n",
    "        harmonic_signal = (sinusoids * amplitudes).sum(dim=-1)\n",
    "        return harmonic_signal\n",
    "\n",
    "class FilteredNoiseGenerator(nn.Module):\n",
    "    def __init__(self, n_magnitudes):\n",
    "        super().__init__()\n",
    "        self.n_magnitudes = n_magnitudes\n",
    "\n",
    "    def forward(self, magnitudes):\n",
    "        batch_size, time_steps, _ = magnitudes.shape\n",
    "        \n",
    "        # Generate white noise\n",
    "        noise = torch.randn(batch_size, time_steps * 256, device=magnitudes.device) # Assume frame size 256\n",
    "\n",
    "        # This is a simplified filtering process\n",
    "        # A real implementation would use FFTs for proper frequency-domain filtering\n",
    "        # For this example, we'll just modulate the noise amplitude\n",
    "        magnitudes_upsampled = F.interpolate(magnitudes.transpose(1, 2), size=noise.shape[1], mode='linear').transpose(1, 2)\n",
    "        noise_signal = noise * magnitudes_upsampled.mean(dim=-1)\n",
    "        return noise_signal\n",
    "\n",
    "class DDSPSynthesizer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_harmonics, n_noise_magnitudes):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.harmonic_amp = nn.Linear(hidden_dim, n_harmonics + 1)\n",
    "        self.noise_mag = nn.Linear(hidden_dim, n_noise_magnitudes)\n",
    "\n",
    "    def forward(self, f0, loudness, mfcc):\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([f0, loudness, mfcc], dim=-1)\n",
    "        \n",
    "        # Pass through GRU\n",
    "        gru_out, _ = self.gru(combined_features)\n",
    "        \n",
    "        # Predict synthesis parameters\n",
    "        harmonic_amplitudes = torch.sigmoid(self.harmonic_amp(gru_out)) # Amplitudes are between 0 and 1\n",
    "        noise_magnitudes = torch.sigmoid(self.noise_mag(gru_out))\n",
    "        \n",
    "        return {'amplitudes': harmonic_amplitudes, 'magnitudes': noise_magnitudes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "new_nca_cell"
   },
   "outputs": [],
   "source": [
    "### --- NEW CELL --- ###\n",
    "# Defines the Neural Cellular Automaton model.\n",
    "\n",
    "class NCA(nn.Module):\n",
    "    def __init__(self, channels, hidden_channels=32):\n",
    "        super().__init__()\n",
    "        # Simple 3-layer CNN to act as the update rule\n",
    "        self.update_rule = nn.Sequential(\n",
    "            nn.Conv1d(channels, hidden_channels, kernel_size=3, padding=1, padding_mode='circular'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_channels, channels, kernel_size=1)\n",
    "        )\n",
    "        # Initialize the final layer to zero so the initial update is zero\n",
    "        # This makes the NCA start by doing nothing to the input seed\n",
    "        self.update_rule[-1].weight.data.zero_()\n",
    "        self.update_rule[-1].bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, steps):\n",
    "        # x is the seed: (batch, time_steps, channels)\n",
    "        x = x.transpose(1, 2) # Conv1d expects (batch, channels, length)\n",
    "        for _ in range(steps):\n",
    "            update = self.update_rule(x)\n",
    "            x = x + update\n",
    "        return x.transpose(1, 2) # Convert back to (batch, time, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjQ3Ie7yNebW"
   },
   "outputs": [],
   "source": [
    "### --- MODIFIED CELL --- ###\n",
    "# Integrated the NCA into the main DDSP model.\n",
    "\n",
    "class DDSPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_harmonics, n_noise_magnitudes, sr):\n",
    "        super().__init__()\n",
    "        self.synthesizer = DDSPSynthesizer(input_dim, hidden_dim, n_harmonics, n_noise_magnitudes)\n",
    "        self.harmonic_osc = HarmonicOscillator(n_harmonics, sr)\n",
    "        self.noise_gen = FilteredNoiseGenerator(n_noise_magnitudes)\n",
    "        \n",
    "        # Add the NCA model as a component\n",
    "        self.nca = NCA(channels=n_harmonics + 1)\n",
    "\n",
    "    def forward(self, features, nca_steps=0):\n",
    "        # 1. DDSP predicts the synthesis parameters (the \"perfect\" seed)\n",
    "        synth_params = self.synthesizer(features['f0'], features['loudness'], features['mfcc'])\n",
    "\n",
    "        harmonic_controls = synth_params['amplitudes']\n",
    "        noise_controls = synth_params['magnitudes']\n",
    "\n",
    "        # 2. If nca_steps > 0, the NCA evolves the harmonic amplitudes\n",
    "        if nca_steps > 0:\n",
    "            harmonic_controls = self.nca(harmonic_controls, steps=nca_steps)\n",
    "            # Ensure output is still in a valid range\n",
    "            harmonic_controls = torch.sigmoid(harmonic_controls)\n",
    "\n",
    "        # 3. The synthesizers generate audio from the (possibly evolved) parameters\n",
    "        harmonic_signal = self.harmonic_osc(features['f0'], harmonic_controls)\n",
    "        noise_signal = self.noise_gen(noise_controls)\n",
    "\n",
    "        # Upsample to match audio length\n",
    "        target_len = audio_tensor.shape[1]\n",
    "        harmonic_signal = F.interpolate(harmonic_signal.unsqueeze(1), size=target_len, mode='linear', align_corners=False).squeeze(1)\n",
    "        noise_signal = F.interpolate(noise_signal.unsqueeze(1), size=target_len, mode='linear', align_corners=False).squeeze(1)\n",
    "\n",
    "        # Mix signals\n",
    "        final_audio = harmonic_signal + noise_signal\n",
    "        return final_audio, harmonic_signal, noise_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0e3v2-MNebW"
   },
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_zYt4X6NebW"
   },
   "outputs": [],
   "source": [
    "# Multi-Scale Spectral Loss\n",
    "def spectral_loss(y_true, y_pred, n_ffts=[2048, 1024, 512, 256]):\n",
    "    loss = 0.0\n",
    "    for n_fft in n_ffts:\n",
    "        stft_true = torch.stft(y_true, n_fft, return_complex=True)\n",
    "        stft_pred = torch.stft(y_pred, n_fft, return_complex=True)\n",
    "        loss += F.l1_loss(torch.abs(stft_true), torch.abs(stft_pred))\n",
    "    return loss\n",
    "\n",
    "# Training Hyperparameters\n",
    "N_HARMONICS = 100\n",
    "N_NOISE_MAGNITUDES = 65\n",
    "HIDDEN_DIM = 256\n",
    "INPUT_DIM = features['f0'].shape[-1] + features['loudness'].shape[-1] + features['mfcc'].shape[-1]\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 1000\n",
    "\n",
    "# Model & Optimizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = DDSPModel(INPUT_DIM, HIDDEN_DIM, N_HARMONICS, N_NOISE_MAGNITUDES, sr).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Move data to device\n",
    "features = {k: v.to(device) for k, v in features.items()}\n",
    "audio_tensor = audio_tensor.to(device)\n",
    "\n",
    "# Training Loop\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "for epoch in pbar:\n",
    "    # During training, we DON'T use the NCA. We want the DDSP to learn a perfect reconstruction.\n",
    "    pred_audio, _, _ = model(features, nca_steps=0)\n",
    "\n",
    "    loss = spectral_loss(audio_tensor, pred_audio)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    pbar.set_description(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_zYt4X6NebW-copy"
   },
   "source": [
    "## 5. Generation: Reconstruct, Grow, and Evolve with NCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Z-d3YkXNebX"
   },
   "outputs": [],
   "source": [
    "### --- MODIFIED CELL --- ###\n",
    "# This cell now generates both the original reconstruction and the new NCA-evolved version.\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 1. Generate the standard DDSP reconstruction (no NCA)\n",
    "    print(\"Generating standard DDSP reconstruction...\")\n",
    "    reconstructed_audio, _, _ = model(features, nca_steps=0)\n",
    "    reconstructed_audio_np = reconstructed_audio.cpu().numpy().flatten()\n",
    "    sf.write('ddsp_reconstructed.wav', reconstructed_audio_np, sr)\n",
    "    print(\" -> Saved ddsp_reconstructed.wav\")\n",
    "\n",
    "    # 2. Generate the NCA-evolved audio\n",
    "    # --- CONTROLS ---\n",
    "    NCA_STEPS = 5 # Try values like 2, 5, 10, or 20. Higher values = more evolution/distortion.\n",
    "    # ----------------\n",
    "    print(f\"\\nGenerating NCA-evolved audio with {NCA_STEPS} steps...\")\n",
    "    evolved_audio, _, _ = model(features, nca_steps=NCA_STEPS)\n",
    "    evolved_audio_np = evolved_audio.cpu().numpy().flatten()\n",
    "    sf.write('nca_evolved_timbre.wav', evolved_audio_np, sr)\n",
    "    print(\" -> Saved nca_evolved_timbre.wav\")\n",
    "\n",
    "    # 3. (Optional) Timbre Growing visualization from the original notebook\n",
    "    # This part is less relevant for the NCA but kept for comparison\n",
    "    print(\"\\nGenerating timbre growth audio (for comparison)...\")\n",
    "    synth_params = model.synthesizer(features['f0'], features['loudness'], features['mfcc'])\n",
    "    original_amplitudes = synth_params['amplitudes']\n",
    "    \n",
    "    full_audio = []\n",
    "    for i in tqdm(range(1, N_HARMONICS + 1, 2)):\n",
    "        temp_amps = torch.zeros_like(original_amplitudes)\n",
    "        temp_amps[:, :, :i] = original_amplitudes[:, :, :i]\n",
    "        \n",
    "        harmonic_signal = model.harmonic_osc(features['f0'], temp_amps)\n",
    "        target_len = int(audio_tensor.shape[1] / 10)\n",
    "        harmonic_signal = F.interpolate(harmonic_signal.unsqueeze(1), size=target_len, mode='linear', align_corners=False).squeeze(1)\n",
    "        full_audio.append(harmonic_signal.cpu().numpy().flatten())\n",
    "    \n",
    "    full_audio = np.concatenate(full_audio)\n",
    "    sf.write('ddsp_growth_comparison.wav', full_audio, sr)\n",
    "    print(\" -> Saved ddsp_growth_comparison.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_zYt4X6NebW-copy2"
   },
   "source": [
    "## 6. Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0e3v2-MNebW-copy"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'features': {k: v.cpu() for k, v in features.items()},\n",
    "}, 'ddsp_nca_model.pt')\n",
    "\n",
    "# Download in Colab\n",
    "print(\"âœ… Files ready for download!\")\n",
    "files.download('ddsp_reconstructed.wav')\n",
    "files.download('nca_evolved_timbre.wav')\n",
    "files.download('ddsp_growth_comparison.wav')\n",
    "files.download('ddsp_nca_model.pt')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}