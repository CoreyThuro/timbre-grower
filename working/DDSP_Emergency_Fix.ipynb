{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP Emergency Fix - Train ONE File at a Time\n",
    "\n",
    "**Problem**: GPU is enabled but mysteriously slow (11.9s per forward pass)\n",
    "\n",
    "**Solution**: Train files sequentially instead of in batch\n",
    "\n",
    "**Expected time**: ~90-120 minutes for 8 files (vs 61 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "!pip install torch librosa soundfile matplotlib scipy tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# DISABLE mixed precision - might be causing slowdown\n",
    "USE_AMP = False\n",
    "print(f\"Mixed precision: {'DISABLED' if not USE_AMP else 'ENABLED'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Upload files\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "os.makedirs('scale_tones', exist_ok=True)\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f'scale_tones/{filename}')\n",
    "\n",
    "audio_files = sorted(glob.glob('scale_tones/*.wav'))\n",
    "print(f\"Uploaded {len(audio_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define DDSP model (SIMPLIFIED - no mixed precision complications)\n",
    "\n",
    "class HarmonicOscillator(nn.Module):\n",
    "    def __init__(self, sample_rate=22050, n_harmonics=64):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_harmonics = n_harmonics\n",
    "\n",
    "    def forward(self, f0_hz, harmonic_amplitudes):\n",
    "        batch_size, n_frames = f0_hz.shape\n",
    "        hop_length = 512\n",
    "        n_samples = n_frames * hop_length\n",
    "\n",
    "        f0_upsampled = F.interpolate(\n",
    "            f0_hz.unsqueeze(1), size=n_samples, mode='linear', align_corners=True\n",
    "        ).squeeze(1)\n",
    "\n",
    "        harmonic_amplitudes_upsampled = F.interpolate(\n",
    "            harmonic_amplitudes.transpose(1, 2), size=n_samples,\n",
    "            mode='linear', align_corners=True\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        phase = 2 * torch.pi * torch.cumsum(f0_upsampled / self.sample_rate, dim=1)\n",
    "        audio = torch.zeros(batch_size, n_samples, device=f0_hz.device)\n",
    "        \n",
    "        for h in range(self.n_harmonics):\n",
    "            harmonic_phase = phase * (h + 1)\n",
    "            harmonic_signal = torch.sin(harmonic_phase) * harmonic_amplitudes_upsampled[:, :, h]\n",
    "            audio += harmonic_signal\n",
    "\n",
    "        return audio\n",
    "\n",
    "\n",
    "class FilteredNoiseGenerator(nn.Module):\n",
    "    def __init__(self, sample_rate=22050, n_filter_banks=64):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_filter_banks = n_filter_banks\n",
    "        self.register_buffer(\n",
    "            'filter_freqs',\n",
    "            torch.logspace(torch.log10(torch.tensor(20.0)), torch.log10(torch.tensor(sample_rate / 2.0)), n_filter_banks)\n",
    "        )\n",
    "\n",
    "    def forward(self, filter_magnitudes):\n",
    "        batch_size, n_frames, _ = filter_magnitudes.shape\n",
    "        hop_length = 512\n",
    "        n_samples = n_frames * hop_length\n",
    "\n",
    "        noise = torch.randn(batch_size, n_samples, device=filter_magnitudes.device)\n",
    "        noise_fft = torch.fft.rfft(noise, dim=1)\n",
    "        freqs = torch.fft.rfftfreq(n_samples, 1/self.sample_rate).to(filter_magnitudes.device)\n",
    "\n",
    "        filter_magnitudes_upsampled = F.interpolate(\n",
    "            filter_magnitudes.transpose(1, 2), size=n_samples,\n",
    "            mode='linear', align_corners=True\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        filter_response = torch.zeros(batch_size, len(freqs), device=filter_magnitudes.device)\n",
    "        for i, freq in enumerate(freqs):\n",
    "            distances = torch.abs(torch.log(self.filter_freqs + 1e-7) - torch.log(freq + 1e-7))\n",
    "            weights = torch.exp(-distances**2 / 0.5)\n",
    "            weights = weights / (weights.sum() + 1e-7)\n",
    "            filter_value = (filter_magnitudes_upsampled.mean(dim=1) * weights).sum(dim=1)\n",
    "            filter_response[:, i] = filter_value\n",
    "\n",
    "        filtered_fft = noise_fft * filter_response\n",
    "        filtered_noise = torch.fft.irfft(filtered_fft, n=n_samples, dim=1)\n",
    "        return filtered_noise\n",
    "\n",
    "\n",
    "class DDSPSynthesizer(nn.Module):\n",
    "    def __init__(self, n_harmonics=64, n_filter_banks=64, hidden_size=512, n_mfcc=30):\n",
    "        super().__init__()\n",
    "        input_size = 1 + 1 + n_mfcc\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        \n",
    "        self.harmonic_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_harmonics),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.noise_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_filter_banks),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, f0, loudness, mfcc):\n",
    "        x = torch.cat([f0, loudness, mfcc], dim=-1)\n",
    "        x, _ = self.gru(x)\n",
    "        harmonic_amplitudes = self.harmonic_head(x)\n",
    "        filter_magnitudes = self.noise_head(x)\n",
    "        loudness_scale = torch.exp(loudness / 20.0)\n",
    "        harmonic_amplitudes = harmonic_amplitudes * loudness_scale\n",
    "        return harmonic_amplitudes, filter_magnitudes\n",
    "\n",
    "\n",
    "class DDSPModel(nn.Module):\n",
    "    def __init__(self, sample_rate=22050, n_harmonics=64, n_filter_banks=64, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.synthesizer = DDSPSynthesizer(n_harmonics, n_filter_banks, hidden_size)\n",
    "        self.harmonic_osc = HarmonicOscillator(sample_rate, n_harmonics)\n",
    "        self.noise_gen = FilteredNoiseGenerator(sample_rate, n_filter_banks)\n",
    "        self.register_parameter('harmonic_noise_ratio', nn.Parameter(torch.tensor(0.8)))\n",
    "\n",
    "    def forward(self, f0, loudness, mfcc):\n",
    "        harmonic_amplitudes, filter_magnitudes = self.synthesizer(f0, loudness, mfcc)\n",
    "        f0_hz = f0.squeeze(-1)\n",
    "        harmonic_audio = self.harmonic_osc(f0_hz, harmonic_amplitudes)\n",
    "        noise_audio = self.noise_gen(filter_magnitudes)\n",
    "        ratio = torch.sigmoid(self.harmonic_noise_ratio)\n",
    "        audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n",
    "        return audio, harmonic_audio, noise_audio\n",
    "\n",
    "\n",
    "class MultiScaleSpectralLoss(nn.Module):\n",
    "    def __init__(self, fft_sizes=[2048, 1024, 512, 256]):\n",
    "        super().__init__()\n",
    "        self.fft_sizes = fft_sizes\n",
    "\n",
    "    def forward(self, pred_audio, target_audio):\n",
    "        total_loss = 0.0\n",
    "        for fft_size in self.fft_sizes:\n",
    "            pred_stft = torch.stft(\n",
    "                pred_audio, n_fft=fft_size, hop_length=fft_size // 4,\n",
    "                window=torch.hann_window(fft_size, device=pred_audio.device),\n",
    "                return_complex=True\n",
    "            )\n",
    "            target_stft = torch.stft(\n",
    "                target_audio, n_fft=fft_size, hop_length=fft_size // 4,\n",
    "                window=torch.hann_window(fft_size, device=target_audio.device),\n",
    "                return_complex=True\n",
    "            )\n",
    "            pred_log_mag = torch.log(torch.abs(pred_stft) + 1e-5)\n",
    "            target_log_mag = torch.log(torch.abs(target_stft) + 1e-5)\n",
    "            total_loss += F.l1_loss(pred_log_mag, target_log_mag)\n",
    "        return total_loss / len(self.fft_sizes)\n",
    "\n",
    "print(\"âœ… Model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature extraction\n",
    "\n",
    "def extract_features(audio_path, sample_rate=22050):\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)\n",
    "    hop_length = int(sample_rate / 43.066)\n",
    "\n",
    "    f0_yin = librosa.yin(audio, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'), sr=sr, hop_length=hop_length)\n",
    "    f0_yin = np.nan_to_num(f0_yin, nan=0.0)\n",
    "    f0_yin = np.maximum(f0_yin, 0.0)\n",
    "\n",
    "    loudness = librosa.feature.rms(y=audio, frame_length=2048, hop_length=hop_length)[0]\n",
    "    loudness_db = librosa.amplitude_to_db(loudness, ref=1.0)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=30, hop_length=hop_length).T\n",
    "    min_len = min(len(f0_yin), len(loudness_db), len(mfcc))\n",
    "\n",
    "    return {\n",
    "        'f0': f0_yin[:min_len],\n",
    "        'loudness': loudness_db[:min_len],\n",
    "        'mfcc': mfcc[:min_len],\n",
    "        'audio': audio,\n",
    "        'n_frames': min_len\n",
    "    }\n",
    "\n",
    "print(\"âœ… Feature extraction ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train ONE FILE AT A TIME (sequential, not batch)\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "print(f\"ðŸŽ¯ Training {len(audio_files)} files SEQUENTIALLY\")\n",
    "print(f\"   Epochs per file: {N_EPOCHS}\")\n",
    "print(f\"   Estimated time per file: 10-15 minutes\")\n",
    "print(f\"   Total estimated time: {len(audio_files) * 12} minutes\\n\")\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for file_idx, audio_file in enumerate(audio_files):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"File {file_idx + 1}/{len(audio_files)}: {Path(audio_file).name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Extract features for this file\n",
    "    print(\"ðŸ“Š Extracting features...\")\n",
    "    features = extract_features(audio_file)\n",
    "    \n",
    "    f0 = torch.tensor(features['f0'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "    loudness = torch.tensor(features['loudness'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "    mfcc = torch.tensor(features['mfcc'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    target_audio = torch.tensor(features['audio'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    print(f\"   F0: {features['f0'].min():.1f} - {features['f0'].max():.1f} Hz\")\n",
    "    print(f\"   Frames: {features['n_frames']}\")\n",
    "    \n",
    "    # Create fresh model for this file\n",
    "    model = DDSPModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = MultiScaleSpectralLoss().to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    file_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nðŸš€ Training...\")\n",
    "    \n",
    "    for epoch in tqdm(range(N_EPOCHS), desc=f\"File {file_idx+1}\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Simple forward pass (NO mixed precision)\n",
    "        pred_audio, _, _ = model(f0, loudness, mfcc)\n",
    "        \n",
    "        min_len = min(pred_audio.shape[1], target_audio.shape[1])\n",
    "        pred_audio_trim = pred_audio[:, :min_len]\n",
    "        target_audio_trim = target_audio[:, :min_len]\n",
    "        \n",
    "        spec_loss = loss_fn(pred_audio_trim, target_audio_trim)\n",
    "        time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n",
    "        total_loss = 1.0 * spec_loss + 0.1 * time_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = total_loss.item()\n",
    "        losses.append(loss_value)\n",
    "        \n",
    "        if loss_value < best_loss:\n",
    "            best_loss = loss_value\n",
    "        \n",
    "        if epoch == 0:\n",
    "            epoch_time = time.time() - file_start_time\n",
    "            print(f\"\\n   First epoch: {epoch_time:.1f}s\")\n",
    "            print(f\"   Loss: {loss_value:.6f}\\n\")\n",
    "        \n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            elapsed = time.time() - file_start_time\n",
    "            print(f\"\\n   Epoch {epoch+1}: Loss={loss_value:.6f}, Best={best_loss:.6f}, Time={elapsed/60:.1f}min\")\n",
    "    \n",
    "    file_time = time.time() - file_start_time\n",
    "    \n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'features': features,\n",
    "        'losses': losses,\n",
    "        'best_loss': best_loss,\n",
    "    }, f\"outputs/model_{Path(audio_file).stem}.pt\")\n",
    "    \n",
    "    print(f\"\\nâœ… File {file_idx + 1} complete!\")\n",
    "    print(f\"   Time: {file_time/60:.1f} minutes\")\n",
    "    print(f\"   Final loss: {best_loss:.6f}\")\n",
    "    print(f\"   Model saved: model_{Path(audio_file).stem}.pt\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "\n",
    "print(f\"\\n\\n{'='*70}\")\n",
    "print(f\"ðŸŽ‰ ALL FILES COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"Average per file: {total_time/60/len(audio_files):.1f} minutes\")\n",
    "print(f\"\\nModels saved in: outputs/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
