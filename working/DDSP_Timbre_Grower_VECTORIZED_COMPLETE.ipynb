{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP Timbre Grower - VECTORIZED COMPLETE üöÄ\n",
    "\n",
    "**THE FIX**: Vectorized HarmonicOscillator and FilteredNoiseGenerator\n",
    "\n",
    "**Root cause**: Python for-loops launching hundreds of sequential GPU kernels\n",
    "\n",
    "**Solution**: Vectorize all loops for parallel GPU execution\n",
    "\n",
    "**Expected**: ~0.5-1s per epoch, 8-17 minutes for 1000 epochs per file\n",
    "\n",
    "**Output**: Growing stages for EACH scale tone file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "!pip install torch librosa soundfile matplotlib scipy tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Upload scale tone files\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "os.makedirs('scale_tones', exist_ok=True)\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f'scale_tones/{filename}')\n",
    "\n",
    "audio_files = sorted(glob.glob('scale_tones/*.wav'))\n",
    "print(f\"\\n‚úÖ Uploaded {len(audio_files)} scale tone files\")\n",
    "for f in audio_files:\n",
    "    print(f\"   - {Path(f).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3. VECTORIZED DDSP Components (THE FIX!)\n\nclass HarmonicOscillatorVectorized(nn.Module):\n    \"\"\"VECTORIZED harmonic oscillator - NO for-loops!\"\"\"\n\n    def __init__(self, sample_rate=22050, n_harmonics=64):\n        super().__init__()\n        self.sample_rate = sample_rate\n        self.n_harmonics = n_harmonics\n        \n        # Pre-compute harmonic numbers [1, 2, 3, ..., n_harmonics]\n        self.register_buffer(\n            'harmonic_numbers',\n            torch.arange(1, n_harmonics + 1, dtype=torch.float32)\n        )\n\n    def forward(self, f0_hz, harmonic_amplitudes):\n        batch_size, n_frames = f0_hz.shape\n        hop_length = 512\n        n_samples = n_frames * hop_length\n\n        # Upsample f0 and amplitudes\n        f0_upsampled = F.interpolate(\n            f0_hz.unsqueeze(1), size=n_samples, mode='linear', align_corners=True\n        ).squeeze(1)  # [batch, samples]\n\n        harmonic_amplitudes_upsampled = F.interpolate(\n            harmonic_amplitudes.transpose(1, 2), size=n_samples,\n            mode='linear', align_corners=True\n        ).transpose(1, 2)  # [batch, samples, n_harmonics]\n\n        # Compute base phase\n        phase = 2 * torch.pi * torch.cumsum(f0_upsampled / self.sample_rate, dim=1)  # [batch, samples]\n\n        # VECTORIZED: Generate ALL harmonics at once!\n        # Broadcast phase to [batch, samples, n_harmonics]\n        phase_broadcast = phase.unsqueeze(-1)  # [batch, samples, 1]\n        harmonic_phases = phase_broadcast * self.harmonic_numbers  # [batch, samples, n_harmonics]\n        \n        # Single sin() call for ALL harmonics\n        harmonic_signals = torch.sin(harmonic_phases)  # [batch, samples, n_harmonics]\n        \n        # Apply amplitudes and sum across harmonics\n        weighted_harmonics = harmonic_signals * harmonic_amplitudes_upsampled\n        audio = weighted_harmonics.sum(dim=-1)  # [batch, samples]\n\n        return audio\n\n\nclass FilteredNoiseGeneratorVectorized(nn.Module):\n    \"\"\"VECTORIZED noise generator - NO for-loops!\"\"\"\n\n    def __init__(self, sample_rate=22050, n_filter_banks=64):\n        super().__init__()\n        self.sample_rate = sample_rate\n        self.n_filter_banks = n_filter_banks\n\n        self.register_buffer(\n            'filter_freqs',\n            torch.logspace(\n                torch.log10(torch.tensor(20.0)),\n                torch.log10(torch.tensor(sample_rate / 2.0)),\n                n_filter_banks\n            )\n        )\n\n    def forward(self, filter_magnitudes):\n        batch_size, n_frames, _ = filter_magnitudes.shape\n        hop_length = 512\n        n_samples = n_frames * hop_length\n\n        # Generate white noise\n        noise = torch.randn(batch_size, n_samples, device=filter_magnitudes.device)\n        noise_fft = torch.fft.rfft(noise, dim=1)\n        freqs = torch.fft.rfftfreq(n_samples, 1/self.sample_rate).to(filter_magnitudes.device)\n\n        # Upsample filter magnitudes\n        filter_magnitudes_upsampled = F.interpolate(\n            filter_magnitudes.transpose(1, 2), size=n_samples,\n            mode='linear', align_corners=True\n        ).transpose(1, 2)  # [batch, samples, n_filter_banks]\n\n        # VECTORIZED: Compute filter response for ALL frequencies at once\n        log_freqs = torch.log(freqs + 1e-7).unsqueeze(-1)  # [n_freqs, 1]\n        log_filter_freqs = torch.log(self.filter_freqs + 1e-7).unsqueeze(0)  # [1, n_filter_banks]\n        \n        # Compute distances between all freq pairs\n        distances = torch.abs(log_freqs - log_filter_freqs)  # [n_freqs, n_filter_banks]\n        \n        # Compute weights for all frequencies\n        weights = torch.exp(-distances**2 / 0.5)  # [n_freqs, n_filter_banks]\n        weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-7)\n        \n        # Average over time first (matching original behavior)\n        filter_mags_time_avg = filter_magnitudes_upsampled.mean(dim=1)  # [batch, n_filter_banks]\n        # Then apply weights for all frequencies at once\n        filter_response = torch.einsum('fk,bk->bf', weights, filter_mags_time_avg)\n\n        # Apply filter\n        filtered_fft = noise_fft * filter_response\n        filtered_noise = torch.fft.irfft(filtered_fft, n=n_samples, dim=1)\n\n        return filtered_noise\n\n\nclass DDSPSynthesizer(nn.Module):\n    \"\"\"Neural network that maps features to synthesis parameters.\"\"\"\n\n    def __init__(self, n_harmonics=64, n_filter_banks=64, hidden_size=512, n_mfcc=30):\n        super().__init__()\n        input_size = 1 + 1 + n_mfcc\n        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=2, batch_first=True, dropout=0.1)\n        \n        self.harmonic_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_harmonics),\n            nn.Softplus()\n        )\n        \n        self.noise_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.LayerNorm(hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, n_filter_banks),\n            nn.Sigmoid()\n        )\n\n    def forward(self, f0, loudness, mfcc):\n        x = torch.cat([f0, loudness, mfcc], dim=-1)\n        x, _ = self.gru(x)\n        harmonic_amplitudes = self.harmonic_head(x)\n        filter_magnitudes = self.noise_head(x)\n        loudness_scale = torch.exp(loudness / 20.0)\n        harmonic_amplitudes = harmonic_amplitudes * loudness_scale\n        return harmonic_amplitudes, filter_magnitudes\n\n\nclass DDSPModelVectorized(nn.Module):\n    \"\"\"Complete DDSP model with VECTORIZED components.\"\"\"\n\n    def __init__(self, sample_rate=22050, n_harmonics=64, n_filter_banks=64, hidden_size=512):\n        super().__init__()\n        self.sample_rate = sample_rate\n        self.synthesizer = DDSPSynthesizer(n_harmonics, n_filter_banks, hidden_size)\n        self.harmonic_osc = HarmonicOscillatorVectorized(sample_rate, n_harmonics)  # VECTORIZED!\n        self.noise_gen = FilteredNoiseGeneratorVectorized(sample_rate, n_filter_banks)  # VECTORIZED!\n        self.register_parameter('harmonic_noise_ratio', nn.Parameter(torch.tensor(0.8)))\n\n    def forward(self, f0, loudness, mfcc):\n        harmonic_amplitudes, filter_magnitudes = self.synthesizer(f0, loudness, mfcc)\n        f0_hz = f0.squeeze(-1)\n        harmonic_audio = self.harmonic_osc(f0_hz, harmonic_amplitudes)\n        noise_audio = self.noise_gen(filter_magnitudes)\n        ratio = torch.sigmoid(self.harmonic_noise_ratio)\n        audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n        return audio, harmonic_audio, noise_audio\n\n\nclass MultiScaleSpectralLoss(nn.Module):\n    \"\"\"Legacy spectral loss (kept for compatibility).\"\"\"\n    def __init__(self, fft_sizes=[2048, 1024, 512, 256]):\n        super().__init__()\n        self.fft_sizes = fft_sizes\n\n    def forward(self, pred_audio, target_audio):\n        total_loss = 0.0\n        for fft_size in self.fft_sizes:\n            pred_stft = torch.stft(\n                pred_audio, n_fft=fft_size, hop_length=fft_size // 4,\n                window=torch.hann_window(fft_size, device=pred_audio.device),\n                return_complex=True\n            )\n            target_stft = torch.stft(\n                target_audio, n_fft=fft_size, hop_length=fft_size // 4,\n                window=torch.hann_window(fft_size, device=target_audio.device),\n                return_complex=True\n            )\n            pred_log_mag = torch.log(torch.abs(pred_stft) + 1e-5)\n            target_log_mag = torch.log(torch.abs(target_stft) + 1e-5)\n            total_loss += F.l1_loss(pred_log_mag, target_log_mag)\n        return total_loss / len(self.fft_sizes)\n\n\nclass MultiResolutionSTFTLoss(nn.Module):\n    \"\"\"Multi-resolution STFT loss for perceptual audio quality (PRIORITY 1 IMPROVEMENT).\"\"\"\n    \n    def __init__(self, fft_sizes=[2048, 1024, 512, 256, 128], \n                 hop_sizes=None, win_sizes=None):\n        super().__init__()\n        if hop_sizes is None:\n            hop_sizes = [s // 4 for s in fft_sizes]\n        if win_sizes is None:\n            win_sizes = fft_sizes\n            \n        self.fft_sizes = fft_sizes\n        self.hop_sizes = hop_sizes\n        self.win_sizes = win_sizes\n        \n    def stft(self, x, fft_size, hop_size, win_size):\n        \"\"\"Compute STFT with given parameters.\"\"\"\n        window = torch.hann_window(win_size).to(x.device)\n        return torch.stft(\n            x, n_fft=fft_size, hop_length=hop_size, \n            win_length=win_size, window=window,\n            return_complex=True\n        )\n    \n    def forward(self, pred_audio, target_audio):\n        spectral_convergence = 0.0\n        magnitude_loss = 0.0\n        \n        for fft_size, hop_size, win_size in zip(\n            self.fft_sizes, self.hop_sizes, self.win_sizes\n        ):\n            pred_stft = self.stft(pred_audio, fft_size, hop_size, win_size)\n            target_stft = self.stft(target_audio, fft_size, hop_size, win_size)\n            \n            # Spectral convergence (measures overall spectral shape similarity)\n            spectral_convergence += torch.norm(\n                target_stft - pred_stft, p='fro'\n            ) / (torch.norm(target_stft, p='fro') + 1e-8)\n            \n            # Log magnitude loss (perceptually weighted)\n            pred_mag = torch.abs(pred_stft)\n            target_mag = torch.abs(target_stft)\n            magnitude_loss += F.l1_loss(\n                torch.log(pred_mag + 1e-5), \n                torch.log(target_mag + 1e-5)\n            )\n        \n        # Average across all resolutions\n        spectral_convergence = spectral_convergence / len(self.fft_sizes)\n        magnitude_loss = magnitude_loss / len(self.fft_sizes)\n        \n        return spectral_convergence + magnitude_loss\n\nprint(\"‚úÖ VECTORIZED model defined with improved perceptual loss\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature extraction\n",
    "\n",
    "def extract_features(audio_path, sample_rate=22050):\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)\n",
    "    hop_length = int(sample_rate / 43.066)\n",
    "\n",
    "    f0_yin = librosa.yin(audio, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'), sr=sr, hop_length=hop_length)\n",
    "    f0_yin = np.nan_to_num(f0_yin, nan=0.0)\n",
    "    f0_yin = np.maximum(f0_yin, 0.0)\n",
    "\n",
    "    loudness = librosa.feature.rms(y=audio, frame_length=2048, hop_length=hop_length)[0]\n",
    "    loudness_db = librosa.amplitude_to_db(loudness, ref=1.0)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=30, hop_length=hop_length).T\n",
    "    min_len = min(len(f0_yin), len(loudness_db), len(mfcc))\n",
    "\n",
    "    return {\n",
    "        'f0': f0_yin[:min_len],\n",
    "        'loudness': loudness_db[:min_len],\n",
    "        'mfcc': mfcc[:min_len],\n",
    "        'audio': audio,\n",
    "        'n_frames': min_len\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Feature extraction ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Growing stage generation functions\n",
    "\n",
    "class ADSREnvelope:\n",
    "    def __init__(self, sr=22050):\n",
    "        self.sr = sr\n",
    "\n",
    "    def generate(self, duration, attack=0.05, decay=0.1, sustain_level=0.7, release=0.2):\n",
    "        n_samples = int(duration * self.sr)\n",
    "        attack_samples = int(attack * self.sr)\n",
    "        decay_samples = int(decay * self.sr)\n",
    "        release_samples = int(release * self.sr)\n",
    "        sustain_samples = n_samples - attack_samples - decay_samples - release_samples\n",
    "        sustain_samples = max(0, sustain_samples)\n",
    "\n",
    "        envelope = []\n",
    "        if attack_samples > 0:\n",
    "            envelope.extend(np.linspace(0, 1, attack_samples))\n",
    "        if decay_samples > 0:\n",
    "            envelope.extend(np.linspace(1, sustain_level, decay_samples))\n",
    "        if sustain_samples > 0:\n",
    "            envelope.extend(np.ones(sustain_samples) * sustain_level)\n",
    "        if release_samples > 0:\n",
    "            envelope.extend(np.linspace(sustain_level, 0, release_samples))\n",
    "\n",
    "        envelope = np.array(envelope)\n",
    "        if len(envelope) < n_samples:\n",
    "            envelope = np.pad(envelope, (0, n_samples - len(envelope)))\n",
    "        elif len(envelope) > n_samples:\n",
    "            envelope = envelope[:n_samples]\n",
    "        return envelope\n",
    "\n",
    "\n",
    "def synthesize_stage(model, features, active_harmonics, duration=0.5, device='cpu'):\n",
    "    \"\"\"Synthesize one stage with progressive harmonic activation.\"\"\"\n",
    "    hop_length = 512\n",
    "    n_frames = int((duration * 22050) / hop_length)\n",
    "\n",
    "    f0_mean = features['f0'][features['f0'] > 0].mean()\n",
    "    if np.isnan(f0_mean):\n",
    "        f0_mean = 220.0\n",
    "\n",
    "    f0_tensor = torch.ones(1, n_frames, 1, device=device) * f0_mean\n",
    "    loudness_tensor = torch.ones(1, n_frames, 1, device=device) * features['loudness'].mean()\n",
    "    mfcc_mean = torch.tensor(features['mfcc'].mean(axis=0), dtype=torch.float32, device=device)\n",
    "    mfcc_tensor = mfcc_mean.unsqueeze(0).unsqueeze(0).expand(1, n_frames, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        harmonic_amplitudes, filter_magnitudes = model.synthesizer(f0_tensor, loudness_tensor, mfcc_tensor)\n",
    "\n",
    "        # Apply harmonic mask\n",
    "        harmonic_mask = torch.tensor(active_harmonics, dtype=torch.float32, device=device)\n",
    "        harmonic_mask = harmonic_mask.unsqueeze(0).unsqueeze(0)\n",
    "        masked_harmonics = harmonic_amplitudes * harmonic_mask\n",
    "\n",
    "        # Generate audio\n",
    "        f0_hz = f0_tensor.squeeze(-1)\n",
    "        harmonic_audio = model.harmonic_osc(f0_hz, masked_harmonics)\n",
    "        noise_audio = model.noise_gen(filter_magnitudes)\n",
    "\n",
    "        ratio = torch.sigmoid(model.harmonic_noise_ratio)\n",
    "        pred_audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n",
    "\n",
    "    audio = pred_audio.squeeze().cpu().numpy()\n",
    "    target_samples = int(duration * 22050)\n",
    "    if len(audio) < target_samples:\n",
    "        audio = np.pad(audio, (0, target_samples - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_samples]\n",
    "\n",
    "    # Apply ADSR envelope\n",
    "    envelope_gen = ADSREnvelope(sr=22050)\n",
    "    envelope = envelope_gen.generate(duration)\n",
    "    audio = audio * envelope\n",
    "\n",
    "    # Normalize\n",
    "    if np.abs(audio).max() > 0:\n",
    "        audio = audio / np.abs(audio).max() * 0.8\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "def generate_stages(strategy='linear', n_harmonics=64):\n",
    "    \"\"\"Generate stage masks.\"\"\"\n",
    "    stages = []\n",
    "    if strategy == 'linear':\n",
    "        for n in range(1, n_harmonics + 1):\n",
    "            stage = np.zeros(n_harmonics)\n",
    "            stage[:n] = 1.0\n",
    "            stages.append(stage)\n",
    "    return stages\n",
    "\n",
    "print(\"‚úÖ Growing stage functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 6. Train and generate growing stages for EACH scale tone\n\nN_EPOCHS = 1000\nSTAGE_DURATION = 0.5\nSILENCE_DURATION = 0.2\n\nos.makedirs('outputs', exist_ok=True)\n\nprint(f\"\\nüéØ Processing {len(audio_files)} scale tone files\")\nprint(f\"   Training: {N_EPOCHS} epochs per file\")\nprint(f\"   Expected: 10-15 minutes per file\")\nprint(f\"   üÜï USING IMPROVED PERCEPTUAL LOSS\\n\")\n\ntotal_start = time.time()\n\nfor file_idx, audio_file in enumerate(audio_files):\n    print(f\"\\n{'='*70}\")\n    print(f\"File {file_idx + 1}/{len(audio_files)}: {Path(audio_file).name}\")\n    print(f\"{'='*70}\\n\")\n    \n    # Extract features\n    print(\"üìä Extracting features...\")\n    features = extract_features(audio_file)\n    print(f\"   F0: {features['f0'][features['f0']>0].mean():.1f} Hz\")\n    print(f\"   Frames: {features['n_frames']}\\n\")\n    \n    f0 = torch.tensor(features['f0'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    loudness = torch.tensor(features['loudness'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n    mfcc = torch.tensor(features['mfcc'], dtype=torch.float32).unsqueeze(0).to(device)\n    target_audio = torch.tensor(features['audio'], dtype=torch.float32).unsqueeze(0).to(device)\n    \n    # Create model\n    print(\"üèóÔ∏è  Training DDSP model...\")\n    model = DDSPModelVectorized().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    \n    # IMPROVED: Use MultiResolutionSTFTLoss instead of MultiScaleSpectralLoss\n    loss_fn = MultiResolutionSTFTLoss().to(device)\n    \n    # Training\n    losses = []\n    best_loss = float('inf')\n    file_start = time.time()\n    \n    for epoch in tqdm(range(N_EPOCHS), desc=\"Training\"):\n        model.train()\n        optimizer.zero_grad()\n        \n        pred_audio, _, _ = model(f0, loudness, mfcc)\n        \n        min_len = min(pred_audio.shape[1], target_audio.shape[1])\n        pred_audio_trim = pred_audio[:, :min_len]\n        target_audio_trim = target_audio[:, :min_len]\n        \n        # IMPROVED: Use perceptual STFT loss + increased time-domain weight\n        stft_loss = loss_fn(pred_audio_trim, target_audio_trim)\n        time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n        \n        # NEW WEIGHTS: Prioritize audio quality over spectrograms\n        # stft_loss captures spectral convergence + log magnitude (perceptual)\n        # time_loss weight increased from 0.1 to 0.5 for better waveform matching\n        total_loss = 1.0 * stft_loss + 0.5 * time_loss\n        \n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        loss_value = total_loss.item()\n        losses.append(loss_value)\n        \n        if loss_value < best_loss:\n            best_loss = loss_value\n        \n        if epoch == 0:\n            print(f\"\\n   First epoch: {time.time() - file_start:.1f}s (Loss: {loss_value:.6f})\\n\")\n    \n    train_time = time.time() - file_start\n    print(f\"\\n‚úÖ Training complete! Time: {train_time/60:.1f}min, Loss: {best_loss:.6f}\\n\")\n    \n    # Test reconstruction\n    model.eval()\n    with torch.no_grad():\n        pred_audio, _, _ = model(f0, loudness, mfcc)\n    pred_audio_np = pred_audio.squeeze().cpu().numpy()\n    \n    print(\"üéµ Reconstructed audio:\")\n    display(Audio(pred_audio_np, rate=22050))\n    \n    # Generate growing stages\n    print(f\"\\nüå± Generating growing stages...\")\n    stages = generate_stages('linear', 64)\n    audio_segments = []\n    silence = np.zeros(int(SILENCE_DURATION * 22050))\n    \n    for i, stage in enumerate(tqdm(stages, desc=\"Stages\")):\n        stage_audio = synthesize_stage(model, features, stage, STAGE_DURATION, device)\n        audio_segments.append(stage_audio)\n        if i < len(stages) - 1:\n            audio_segments.append(silence)\n    \n    full_audio = np.concatenate(audio_segments)\n    \n    print(f\"\\n‚úÖ Generated {len(stages)} growing stages\")\n    print(f\"   Duration: {len(full_audio)/22050:.2f}s\\n\")\n    \n    print(\"üéß Growing stages:\")\n    display(Audio(full_audio, rate=22050))\n    \n    # Save files\n    file_stem = Path(audio_file).stem\n    sf.write(f'outputs/{file_stem}_reconstructed.wav', pred_audio_np, 22050)\n    sf.write(f'outputs/{file_stem}_grown.wav', full_audio, 22050)\n    \n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'features': features,\n        'losses': losses,\n        'best_loss': best_loss,\n    }, f'outputs/{file_stem}_model.pt')\n    \n    print(f\"üíæ Saved:\")\n    print(f\"   - {file_stem}_reconstructed.wav\")\n    print(f\"   - {file_stem}_grown.wav\")\n    print(f\"   - {file_stem}_model.pt\")\n\ntotal_time = time.time() - total_start\n\nprint(f\"\\n\\n{'='*70}\")\nprint(f\"üéâ ALL SCALE TONES COMPLETE!\")\nprint(f\"{'='*70}\")\nprint(f\"Total time: {total_time/60:.1f} minutes\")\nprint(f\"Average per file: {total_time/60/len(audio_files):.1f} minutes\")\nprint(f\"\\nAll files saved in: outputs/\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Download all results\n",
    "\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Create zip file\n",
    "with zipfile.ZipFile('ddsp_grown_scale_tones.zip', 'w') as zipf:\n",
    "    for file in glob.glob('outputs/*'):\n",
    "        zipf.write(file, Path(file).name)\n",
    "\n",
    "print(\"üì¶ Downloading results...\")\n",
    "files.download('ddsp_grown_scale_tones.zip')\n",
    "\n",
    "print(\"\\n‚úÖ Complete! All scale tones have been grown and downloaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}