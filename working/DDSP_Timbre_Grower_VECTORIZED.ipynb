{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP Timbre Grower - VECTORIZED (FAST!) 🚀\n",
    "\n",
    "**THE FIX**: Vectorized HarmonicOscillator and FilteredNoiseGenerator\n",
    "\n",
    "**Root cause**: Python for-loops launching hundreds of sequential GPU kernels\n",
    "\n",
    "**Solution**: Vectorize all loops for parallel GPU execution\n",
    "\n",
    "**Expected**: ~0.5-1s per epoch, 8-17 minutes for 1000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "!pip install torch librosa soundfile matplotlib scipy tqdm -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Upload files\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "os.makedirs('scale_tones', exist_ok=True)\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f'scale_tones/{filename}')\n",
    "\n",
    "audio_files = sorted(glob.glob('scale_tones/*.wav'))\n",
    "print(f\"\\n✅ Uploaded {len(audio_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. VECTORIZED DDSP Components (THE FIX!)\n",
    "\n",
    "class HarmonicOscillatorVectorized(nn.Module):\n",
    "    \"\"\"VECTORIZED harmonic oscillator - NO for-loops!\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=22050, n_harmonics=64):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_harmonics = n_harmonics\n",
    "        \n",
    "        # Pre-compute harmonic numbers [1, 2, 3, ..., n_harmonics]\n",
    "        self.register_buffer(\n",
    "            'harmonic_numbers',\n",
    "            torch.arange(1, n_harmonics + 1, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, f0_hz, harmonic_amplitudes):\n",
    "        batch_size, n_frames = f0_hz.shape\n",
    "        hop_length = 512\n",
    "        n_samples = n_frames * hop_length\n",
    "\n",
    "        # Upsample f0 and amplitudes\n",
    "        f0_upsampled = F.interpolate(\n",
    "            f0_hz.unsqueeze(1), size=n_samples, mode='linear', align_corners=True\n",
    "        ).squeeze(1)  # [batch, samples]\n",
    "\n",
    "        harmonic_amplitudes_upsampled = F.interpolate(\n",
    "            harmonic_amplitudes.transpose(1, 2), size=n_samples,\n",
    "            mode='linear', align_corners=True\n",
    "        ).transpose(1, 2)  # [batch, samples, n_harmonics]\n",
    "\n",
    "        # Compute base phase\n",
    "        phase = 2 * torch.pi * torch.cumsum(f0_upsampled / self.sample_rate, dim=1)  # [batch, samples]\n",
    "\n",
    "        # VECTORIZED: Generate ALL harmonics at once!\n",
    "        # Broadcast phase to [batch, samples, n_harmonics]\n",
    "        phase_broadcast = phase.unsqueeze(-1)  # [batch, samples, 1]\n",
    "        harmonic_phases = phase_broadcast * self.harmonic_numbers  # [batch, samples, n_harmonics]\n",
    "        \n",
    "        # Single sin() call for ALL harmonics\n",
    "        harmonic_signals = torch.sin(harmonic_phases)  # [batch, samples, n_harmonics]\n",
    "        \n",
    "        # Apply amplitudes and sum across harmonics\n",
    "        weighted_harmonics = harmonic_signals * harmonic_amplitudes_upsampled\n",
    "        audio = weighted_harmonics.sum(dim=-1)  # [batch, samples]\n",
    "\n",
    "        return audio\n",
    "\n",
    "\n",
    "class FilteredNoiseGeneratorVectorized(nn.Module):\n",
    "    \"\"\"VECTORIZED noise generator - NO for-loops!\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=22050, n_filter_banks=64):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_filter_banks = n_filter_banks\n",
    "\n",
    "        self.register_buffer(\n",
    "            'filter_freqs',\n",
    "            torch.logspace(\n",
    "                torch.log10(torch.tensor(20.0)),\n",
    "                torch.log10(torch.tensor(sample_rate / 2.0)),\n",
    "                n_filter_banks\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, filter_magnitudes):\n",
    "        batch_size, n_frames, _ = filter_magnitudes.shape\n",
    "        hop_length = 512\n",
    "        n_samples = n_frames * hop_length\n",
    "\n",
    "        # Generate white noise\n",
    "        noise = torch.randn(batch_size, n_samples, device=filter_magnitudes.device)\n",
    "        noise_fft = torch.fft.rfft(noise, dim=1)\n",
    "        freqs = torch.fft.rfftfreq(n_samples, 1/self.sample_rate).to(filter_magnitudes.device)\n",
    "\n",
    "        # Upsample filter magnitudes\n",
    "        filter_magnitudes_upsampled = F.interpolate(\n",
    "            filter_magnitudes.transpose(1, 2), size=n_samples,\n",
    "            mode='linear', align_corners=True\n",
    "        ).transpose(1, 2)  # [batch, samples, n_filter_banks]\n",
    "\n",
    "        # VECTORIZED: Compute filter response for ALL frequencies at once\n",
    "        # Shape manipulations for broadcasting\n",
    "        log_freqs = torch.log(freqs + 1e-7).unsqueeze(-1)  # [n_freqs, 1]\n",
    "        log_filter_freqs = torch.log(self.filter_freqs + 1e-7).unsqueeze(0)  # [1, n_filter_banks]\n",
    "        \n",
    "        # Compute distances between all freq pairs\n",
    "        distances = torch.abs(log_freqs - log_filter_freqs)  # [n_freqs, n_filter_banks]\n",
    "        \n",
    "        # Compute weights for all frequencies\n",
    "        weights = torch.exp(-distances**2 / 0.5)  # [n_freqs, n_filter_banks]\n",
    "        weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-7)\n",
    "        \n",
    "        # Apply weights using batch matrix multiplication\n",
    "        # filter_magnitudes_upsampled: [batch, samples, n_filter_banks]\n",
    "        # We want: [batch, n_freqs]\n",
    "        filter_response = torch.einsum('fk,bsk->bf', weights, filter_magnitudes_upsampled)\n",
    "\n",
    "        # Apply filter\n",
    "        filtered_fft = noise_fft * filter_response\n",
    "        filtered_noise = torch.fft.irfft(filtered_fft, n=n_samples, dim=1)\n",
    "\n",
    "        return filtered_noise\n",
    "\n",
    "\n",
    "class DDSPSynthesizer(nn.Module):\n",
    "    \"\"\"Neural network that maps features to synthesis parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, n_harmonics=64, n_filter_banks=64, hidden_size=512, n_mfcc=30):\n",
    "        super().__init__()\n",
    "        input_size = 1 + 1 + n_mfcc\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        \n",
    "        self.harmonic_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_harmonics),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "        self.noise_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_filter_banks),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, f0, loudness, mfcc):\n",
    "        x = torch.cat([f0, loudness, mfcc], dim=-1)\n",
    "        x, _ = self.gru(x)\n",
    "        harmonic_amplitudes = self.harmonic_head(x)\n",
    "        filter_magnitudes = self.noise_head(x)\n",
    "        loudness_scale = torch.exp(loudness / 20.0)\n",
    "        harmonic_amplitudes = harmonic_amplitudes * loudness_scale\n",
    "        return harmonic_amplitudes, filter_magnitudes\n",
    "\n",
    "\n",
    "class DDSPModelVectorized(nn.Module):\n",
    "    \"\"\"Complete DDSP model with VECTORIZED components.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=22050, n_harmonics=64, n_filter_banks=64, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.synthesizer = DDSPSynthesizer(n_harmonics, n_filter_banks, hidden_size)\n",
    "        self.harmonic_osc = HarmonicOscillatorVectorized(sample_rate, n_harmonics)  # VECTORIZED!\n",
    "        self.noise_gen = FilteredNoiseGeneratorVectorized(sample_rate, n_filter_banks)  # VECTORIZED!\n",
    "        self.register_parameter('harmonic_noise_ratio', nn.Parameter(torch.tensor(0.8)))\n",
    "\n",
    "    def forward(self, f0, loudness, mfcc):\n",
    "        harmonic_amplitudes, filter_magnitudes = self.synthesizer(f0, loudness, mfcc)\n",
    "        f0_hz = f0.squeeze(-1)\n",
    "        harmonic_audio = self.harmonic_osc(f0_hz, harmonic_amplitudes)\n",
    "        noise_audio = self.noise_gen(filter_magnitudes)\n",
    "        ratio = torch.sigmoid(self.harmonic_noise_ratio)\n",
    "        audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n",
    "        return audio, harmonic_audio, noise_audio\n",
    "\n",
    "\n",
    "class MultiScaleSpectralLoss(nn.Module):\n",
    "    def __init__(self, fft_sizes=[2048, 1024, 512, 256]):\n",
    "        super().__init__()\n",
    "        self.fft_sizes = fft_sizes\n",
    "\n",
    "    def forward(self, pred_audio, target_audio):\n",
    "        total_loss = 0.0\n",
    "        for fft_size in self.fft_sizes:\n",
    "            pred_stft = torch.stft(\n",
    "                pred_audio, n_fft=fft_size, hop_length=fft_size // 4,\n",
    "                window=torch.hann_window(fft_size, device=pred_audio.device),\n",
    "                return_complex=True\n",
    "            )\n",
    "            target_stft = torch.stft(\n",
    "                target_audio, n_fft=fft_size, hop_length=fft_size // 4,\n",
    "                window=torch.hann_window(fft_size, device=target_audio.device),\n",
    "                return_complex=True\n",
    "            )\n",
    "            pred_log_mag = torch.log(torch.abs(pred_stft) + 1e-5)\n",
    "            target_log_mag = torch.log(torch.abs(target_stft) + 1e-5)\n",
    "            total_loss += F.l1_loss(pred_log_mag, target_log_mag)\n",
    "        return total_loss / len(self.fft_sizes)\n",
    "\n",
    "print(\"✅ VECTORIZED model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature extraction\n",
    "\n",
    "def extract_features(audio_path, sample_rate=22050):\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)\n",
    "    hop_length = int(sample_rate / 43.066)\n",
    "\n",
    "    f0_yin = librosa.yin(audio, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'), sr=sr, hop_length=hop_length)\n",
    "    f0_yin = np.nan_to_num(f0_yin, nan=0.0)\n",
    "    f0_yin = np.maximum(f0_yin, 0.0)\n",
    "\n",
    "    loudness = librosa.feature.rms(y=audio, frame_length=2048, hop_length=hop_length)[0]\n",
    "    loudness_db = librosa.amplitude_to_db(loudness, ref=1.0)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=30, hop_length=hop_length).T\n",
    "    min_len = min(len(f0_yin), len(loudness_db), len(mfcc))\n",
    "\n",
    "    return {\n",
    "        'f0': f0_yin[:min_len],\n",
    "        'loudness': loudness_db[:min_len],\n",
    "        'mfcc': mfcc[:min_len],\n",
    "        'audio': audio,\n",
    "        'n_frames': min_len\n",
    "    }\n",
    "\n",
    "print(\"✅ Feature extraction ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Test vectorized model speed\n",
    "\n",
    "print(\"🧪 Testing vectorized model performance...\\n\")\n",
    "\n",
    "# Load one file for testing\n",
    "test_features = extract_features(audio_files[0])\n",
    "f0_test = torch.tensor(test_features['f0'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "loudness_test = torch.tensor(test_features['loudness'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "mfcc_test = torch.tensor(test_features['mfcc'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "model_test = DDSPModelVectorized().to(device)\n",
    "\n",
    "# Warmup\n",
    "with torch.no_grad():\n",
    "    _ = model_test(f0_test, loudness_test, mfcc_test)\n",
    "\n",
    "# Time forward pass\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    _ = model_test(f0_test, loudness_test, mfcc_test)\n",
    "forward_time = time.time() - start\n",
    "\n",
    "print(f\"Forward pass time: {forward_time:.3f}s\")\n",
    "\n",
    "if forward_time < 0.5:\n",
    "    print(f\"✅ EXCELLENT! This is {11.9 / forward_time:.1f}x faster than before!\")\n",
    "    print(f\"   Expected time per epoch: ~{forward_time * 2:.1f}s\")\n",
    "    print(f\"   Expected total time (1000 epochs): ~{forward_time * 2000 / 60:.1f} minutes\")\n",
    "elif forward_time < 2.0:\n",
    "    print(f\"✅ GOOD! This is {11.9 / forward_time:.1f}x faster than before!\")\n",
    "    print(f\"   Expected time per epoch: ~{forward_time * 2:.1f}s\")\n",
    "    print(f\"   Expected total time (1000 epochs): ~{forward_time * 2000 / 60:.1f} minutes\")\n",
    "else:\n",
    "    print(f\"⚠️  Still slow ({forward_time:.1f}s), but {11.9 / forward_time:.1f}x improvement\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train files sequentially with vectorized model\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "print(f\"\\n🎯 Training {len(audio_files)} files with VECTORIZED model\")\n",
    "print(f\"   Epochs per file: {N_EPOCHS}\")\n",
    "print(f\"   Expected per file: 10-15 minutes\\n\")\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for file_idx, audio_file in enumerate(audio_files):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"File {file_idx + 1}/{len(audio_files)}: {Path(audio_file).name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(audio_file)\n",
    "    \n",
    "    f0 = torch.tensor(features['f0'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "    loudness = torch.tensor(features['loudness'], dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "    mfcc = torch.tensor(features['mfcc'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    target_audio = torch.tensor(features['audio'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Create model\n",
    "    model = DDSPModelVectorized().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = MultiScaleSpectralLoss().to(device)\n",
    "    \n",
    "    # Training\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    file_start = time.time()\n",
    "    \n",
    "    for epoch in tqdm(range(N_EPOCHS), desc=f\"Training\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred_audio, _, _ = model(f0, loudness, mfcc)\n",
    "        \n",
    "        min_len = min(pred_audio.shape[1], target_audio.shape[1])\n",
    "        pred_audio_trim = pred_audio[:, :min_len]\n",
    "        target_audio_trim = target_audio[:, :min_len]\n",
    "        \n",
    "        spec_loss = loss_fn(pred_audio_trim, target_audio_trim)\n",
    "        time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n",
    "        total_loss = 1.0 * spec_loss + 0.1 * time_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_value = total_loss.item()\n",
    "        losses.append(loss_value)\n",
    "        \n",
    "        if loss_value < best_loss:\n",
    "            best_loss = loss_value\n",
    "        \n",
    "        if epoch == 0:\n",
    "            print(f\"\\n   First epoch: {time.time() - file_start:.1f}s (Loss: {loss_value:.6f})\\n\")\n",
    "        \n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            elapsed = time.time() - file_start\n",
    "            eta = (elapsed / (epoch + 1)) * (N_EPOCHS - epoch - 1)\n",
    "            print(f\"\\n   Epoch {epoch+1}: Loss={loss_value:.6f}, Best={best_loss:.6f}, ETA={eta/60:.1f}min\")\n",
    "    \n",
    "    file_time = time.time() - file_start\n",
    "    \n",
    "    # Save\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'features': features,\n",
    "        'losses': losses,\n",
    "        'best_loss': best_loss,\n",
    "    }, f\"outputs/model_{Path(audio_file).stem}.pt\")\n",
    "    \n",
    "    print(f\"\\n✅ Complete! Time: {file_time/60:.1f}min, Loss: {best_loss:.6f}\")\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n\\n🎉 ALL FILES COMPLETE!\")\n",
    "print(f\"Total: {total_time/60:.1f}min, Avg: {total_time/60/len(audio_files):.1f}min per file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
