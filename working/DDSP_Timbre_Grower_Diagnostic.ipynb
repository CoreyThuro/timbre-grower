{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP Diagnostic - Find the Bottleneck\n",
    "\n",
    "Run this to diagnose why training is so slow (264s per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIAGNOSTIC REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\n1. Device Check:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  WARNING: Running on CPU! This is why it's slow.\")\n",
    "\n",
    "# 2. Check where tensors are\n",
    "print(f\"\\n2. Tensor Location Check:\")\n",
    "print(f\"   f0 device: {f0.device}\")\n",
    "print(f\"   model device: {next(model.parameters()).device}\")\n",
    "if str(f0.device) != device or str(next(model.parameters()).device) != device:\n",
    "    print(\"   ‚ö†Ô∏è  WARNING: Tensors not on same device as model!\")\n",
    "\n",
    "# 3. Check batch size\n",
    "print(f\"\\n3. Batch Size Check:\")\n",
    "print(f\"   Batch size: {f0.shape[0]}\")\n",
    "print(f\"   Frames per sample: {f0.shape[1]}\")\n",
    "print(f\"   Total elements: {f0.numel():,}\")\n",
    "if f0.shape[0] > 12:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Batch size {f0.shape[0]} might be too large!\")\n",
    "\n",
    "# 4. Time a single forward pass\n",
    "print(f\"\\n4. Forward Pass Timing:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Warmup\n",
    "    _ = model(f0[:1], loudness[:1], mfcc[:1])\n",
    "    \n",
    "    # Time single sample\n",
    "    start = time.time()\n",
    "    _ = model(f0[:1], loudness[:1], mfcc[:1])\n",
    "    single_time = time.time() - start\n",
    "    \n",
    "    # Time full batch\n",
    "    start = time.time()\n",
    "    _ = model(f0, loudness, mfcc)\n",
    "    batch_time = time.time() - start\n",
    "\n",
    "print(f\"   Single sample: {single_time:.3f}s\")\n",
    "print(f\"   Full batch ({f0.shape[0]} samples): {batch_time:.3f}s\")\n",
    "print(f\"   Expected per-sample cost: {batch_time / f0.shape[0]:.3f}s\")\n",
    "\n",
    "if single_time > 1.0:\n",
    "    print(f\"   ‚ö†Ô∏è  CRITICAL: Single forward pass is {single_time:.1f}s - should be <0.1s!\")\n",
    "    print(f\"   Likely running on CPU or model not optimized.\")\n",
    "\n",
    "# 5. Check mixed precision\n",
    "print(f\"\\n5. Mixed Precision Check:\")\n",
    "try:\n",
    "    from torch.amp import autocast, GradScaler\n",
    "    scaler = GradScaler('cuda')\n",
    "    print(\"   ‚úÖ PyTorch 2.x AMP API detected\")\n",
    "except:\n",
    "    try:\n",
    "        from torch.cuda.amp import autocast, GradScaler\n",
    "        scaler = GradScaler()\n",
    "        print(\"   ‚úÖ PyTorch 1.x AMP API detected\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  WARNING: AMP not available: {e}\")\n",
    "\n",
    "# 6. Memory check\n",
    "if device == 'cuda':\n",
    "    print(f\"\\n6. GPU Memory:\")\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Reserved: {reserved:.2f} GB\")\n",
    "    if allocated > 20:\n",
    "        print(f\"   ‚ö†Ô∏è  WARNING: High memory usage, may cause slowdown\")\n",
    "\n",
    "# 7. Recommendation\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if device == 'cpu':\n",
    "    print(\"\\nüö® CRITICAL: You're running on CPU!\")\n",
    "    print(\"   Solution: Enable GPU in Colab\")\n",
    "    print(\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n",
    "elif single_time > 1.0:\n",
    "    print(\"\\nüö® CRITICAL: Forward pass too slow\")\n",
    "    print(\"   Current: {:.1f}s per forward pass\".format(single_time))\n",
    "    print(\"   Expected: <0.1s\")\n",
    "    print(\"   \\nPossible causes:\")\n",
    "    print(\"   1. Mixed precision not working\")\n",
    "    print(\"   2. Model or tensors not on GPU\")\n",
    "    print(\"   3. GPU utilization low\")\n",
    "elif f0.shape[0] > 12:\n",
    "    print(f\"\\n‚ö†Ô∏è  Batch size ({f0.shape[0]}) might be too large\")\n",
    "    print(\"   Solution: Reduce to 6-8 files\")\n",
    "    print(\"   audio_files = audio_files[:8]\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ System looks OK, but still slow\")\n",
    "    print(\"   Try reducing batch size to 4 files\")\n",
    "    print(\"   Or train files sequentially (1 at a time)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
