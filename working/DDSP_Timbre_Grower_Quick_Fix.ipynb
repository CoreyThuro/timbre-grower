{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDSP Timbre Grower - Quick Fix Version\n",
    "\n",
    "**Fixes Applied**:\n",
    "- ‚úÖ Removed torch.compile (causes hanging)\n",
    "- ‚úÖ Fixed autocast() device_type parameter\n",
    "- ‚úÖ Simplified to core optimizations only\n",
    "- ‚úÖ Added early progress indicators\n",
    "\n",
    "**Expected Performance**: ~60-90 minutes for 1000 epochs with multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch librosa soundfile matplotlib scipy tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal as scipy_signal\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio, display\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Check PyTorch version and import accordingly\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.__version__.startswith('2.'):\n",
    "    from torch.amp import autocast, GradScaler\n",
    "    DEVICE_TYPE = 'cuda'\n",
    "    print(\"Using PyTorch 2.x AMP API\")\n",
    "else:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    DEVICE_TYPE = None  # Old API doesn't need device_type\n",
    "    print(\"Using PyTorch 1.x AMP API\")\n",
    "\n",
    "# Check for GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Target Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab - upload multiple files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üìÅ Upload your scale tone audio files:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Create directory for uploaded files\n",
    "os.makedirs('scale_tones', exist_ok=True)\n",
    "\n",
    "# Move uploaded files to directory\n",
    "for filename in uploaded.keys():\n",
    "    os.rename(filename, f'scale_tones/{filename}')\n",
    "\n",
    "audio_files = sorted(glob.glob('scale_tones/*.wav'))\n",
    "print(f\"\\n‚úÖ Uploaded {len(audio_files)} files:\")\n",
    "for f in audio_files:\n",
    "    print(f\"   - {Path(f).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DDSP Core Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarmonicOscillator(nn.Module):\n",
    "    \"\"\"Differentiable harmonic oscillator for additive synthesis.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=22050, n_harmonics=64):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_harmonics = n_harmonics\n",
    "\n",
    "    def forward(self, f0_hz, harmonic_amplitudes):\n",
    "        batch_size, n_frames = f0_hz.shape\n",
    "        hop_length = 512\n",
    "        n_samples = n_frames * hop_length\n",
    "\n",
    "        f0_upsampled = F.interpolate(\n",
    "            f0_hz.unsqueeze(1), size=n_samples, mode='linear', align_corners=True\n",
    "        ).squeeze(1)\n",
    "\n",
    "        harmonic_amplitudes_upsampled = F.interpolate(\n",
    "            harmonic_amplitudes.transpose(1, 2), size=n_samples,\n",
    "            mode='linear', align_corners=True\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        phase = 2 * torch.pi * torch.cumsum(f0_upsampled / self.sample_rate, dim=1)\n",
    "\n",
    "        audio = torch.zeros(batch_size, n_samples, device=f0_hz.device)\n",
    "        for h in range(self.n_harmonics):\n",
    "            harmonic_phase = phase * (h + 1)\n",
    "            harmonic_signal = torch.sin(harmonic_phase)\n",
    "            harmonic_signal = harmonic_signal * harmonic_amplitudes_upsampled[:, :, h]\n",
    "            audio += harmonic_signal\n",
    "\n",
    "        return audio\n",
    "\n",
    "\n",
    "class FilteredNoiseGenerator(nn.Module):\n",
    "    \"\"\"Differentiable filtered noise generator.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=22050, n_filter_banks=64):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_filter_banks = n_filter_banks\n",
    "\n",
    "        self.register_buffer(\n",
    "            'filter_freqs',\n",
    "            torch.logspace(torch.log10(torch.tensor(20.0)), torch.log10(torch.tensor(sample_rate / 2.0)), n_filter_banks)\n",
    "        )\n",
    "\n",
    "    def forward(self, filter_magnitudes):\n",
    "        batch_size, n_frames, _ = filter_magnitudes.shape\n",
    "        hop_length = 512\n",
    "        n_samples = n_frames * hop_length\n",
    "\n",
    "        noise = torch.randn(batch_size, n_samples, device=filter_magnitudes.device)\n",
    "        noise_fft = torch.fft.rfft(noise, dim=1)\n",
    "        freqs = torch.fft.rfftfreq(n_samples, 1/self.sample_rate).to(filter_magnitudes.device)\n",
    "\n",
    "        filter_magnitudes_upsampled = F.interpolate(\n",
    "            filter_magnitudes.transpose(1, 2), size=n_samples,\n",
    "            mode='linear', align_corners=True\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        filter_response = torch.zeros(batch_size, len(freqs), device=filter_magnitudes.device)\n",
    "        for i, freq in enumerate(freqs):\n",
    "            distances = torch.abs(torch.log(self.filter_freqs + 1e-7) - torch.log(freq + 1e-7))\n",
    "            weights = torch.exp(-distances**2 / 0.5)\n",
    "            weights = weights / (weights.sum() + 1e-7)\n",
    "            filter_value = (filter_magnitudes_upsampled.mean(dim=1) * weights).sum(dim=1)\n",
    "            filter_response[:, i] = filter_value\n",
    "\n",
    "        filtered_fft = noise_fft * filter_response\n",
    "        filtered_noise = torch.fft.irfft(filtered_fft, n=n_samples, dim=1)\n",
    "\n",
    "        return filtered_noise\n",
    "\n",
    "\n",
    "class DDSPSynthesizer(nn.Module):\n",
    "    \"\"\"Neural network that maps features to synthesis parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, n_harmonics=64, n_filter_banks=64, hidden_size=512, n_mfcc=30):\n",
    "        super().__init__()\n",
    "        self.n_harmonics = n_harmonics\n",
    "        self.n_filter_banks = n_filter_banks\n",
    "\n",
    "        input_size = 1 + 1 + n_mfcc\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size, hidden_size=hidden_size,\n",
    "            num_layers=2, batch_first=True, dropout=0.1\n",
    "        )\n",
    "\n",
    "        self.harmonic_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_harmonics),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "        self.noise_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_filter_banks),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, f0, loudness, mfcc):\n",
    "        x = torch.cat([f0, loudness, mfcc], dim=-1)\n",
    "        x, _ = self.gru(x)\n",
    "\n",
    "        harmonic_amplitudes = self.harmonic_head(x)\n",
    "        filter_magnitudes = self.noise_head(x)\n",
    "\n",
    "        loudness_scale = torch.exp(loudness / 20.0)\n",
    "        harmonic_amplitudes = harmonic_amplitudes * loudness_scale\n",
    "\n",
    "        return harmonic_amplitudes, filter_magnitudes\n",
    "\n",
    "\n",
    "class DDSPModel(nn.Module):\n",
    "    \"\"\"Complete DDSP model.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate=22050, n_harmonics=64, n_filter_banks=64, hidden_size=512):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        self.synthesizer = DDSPSynthesizer(n_harmonics, n_filter_banks, hidden_size)\n",
    "        self.harmonic_osc = HarmonicOscillator(sample_rate, n_harmonics)\n",
    "        self.noise_gen = FilteredNoiseGenerator(sample_rate, n_filter_banks)\n",
    "\n",
    "        self.register_parameter(\n",
    "            'harmonic_noise_ratio', nn.Parameter(torch.tensor(0.8))\n",
    "        )\n",
    "\n",
    "    def forward(self, f0, loudness, mfcc):\n",
    "        harmonic_amplitudes, filter_magnitudes = self.synthesizer(f0, loudness, mfcc)\n",
    "\n",
    "        f0_hz = f0.squeeze(-1)\n",
    "        harmonic_audio = self.harmonic_osc(f0_hz, harmonic_amplitudes)\n",
    "        noise_audio = self.noise_gen(filter_magnitudes)\n",
    "\n",
    "        ratio = torch.sigmoid(self.harmonic_noise_ratio)\n",
    "        audio = ratio * harmonic_audio + (1 - ratio) * noise_audio\n",
    "\n",
    "        return audio, harmonic_audio, noise_audio\n",
    "\n",
    "\n",
    "class OptimizedMultiScaleSpectralLoss(nn.Module):\n",
    "    \"\"\"Multi-scale spectral loss with cached target computation.\"\"\"\n",
    "\n",
    "    def __init__(self, fft_sizes=[2048, 1024, 512, 256]):\n",
    "        super().__init__()\n",
    "        self.fft_sizes = fft_sizes\n",
    "        self.target_stfts = {}\n",
    "\n",
    "    def precompute_target(self, target_audio, device):\n",
    "        \"\"\"Precompute target STFTs once before training.\"\"\"\n",
    "        print(\"üìä Precomputing target STFTs...\")\n",
    "        with torch.no_grad():\n",
    "            for fft_size in self.fft_sizes:\n",
    "                target_stft = torch.stft(\n",
    "                    target_audio, n_fft=fft_size, hop_length=fft_size // 4,\n",
    "                    window=torch.hann_window(fft_size, device=device),\n",
    "                    return_complex=True\n",
    "                )\n",
    "                target_log_mag = torch.log(torch.abs(target_stft) + 1e-5)\n",
    "                self.target_stfts[fft_size] = target_log_mag\n",
    "        print(\"‚úÖ Target STFTs cached\")\n",
    "\n",
    "    def forward(self, pred_audio):\n",
    "        \"\"\"Compute loss using cached target STFTs.\"\"\"\n",
    "        total_loss = 0.0\n",
    "        device = pred_audio.device\n",
    "\n",
    "        for fft_size in self.fft_sizes:\n",
    "            pred_stft = torch.stft(\n",
    "                pred_audio, n_fft=fft_size, hop_length=fft_size // 4,\n",
    "                window=torch.hann_window(fft_size, device=device),\n",
    "                return_complex=True\n",
    "            )\n",
    "\n",
    "            pred_log_mag = torch.log(torch.abs(pred_stft) + 1e-5)\n",
    "            total_loss += F.l1_loss(pred_log_mag, self.target_stfts[fft_size])\n",
    "\n",
    "        return total_loss / len(self.fft_sizes)\n",
    "\n",
    "\n",
    "print(\"‚úÖ DDSP components defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-File Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path, sample_rate=22050):\n",
    "    \"\"\"Extract f0, loudness, and MFCCs from audio.\"\"\"\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)\n",
    "    hop_length = int(sample_rate / 43.066)\n",
    "\n",
    "    f0_yin = librosa.yin(\n",
    "        audio, fmin=librosa.note_to_hz('C2'),\n",
    "        fmax=librosa.note_to_hz('C7'), sr=sr, hop_length=hop_length\n",
    "    )\n",
    "    f0_yin = np.nan_to_num(f0_yin, nan=0.0)\n",
    "    f0_yin = np.maximum(f0_yin, 0.0)\n",
    "\n",
    "    loudness = librosa.feature.rms(\n",
    "        y=audio, frame_length=2048, hop_length=hop_length\n",
    "    )[0]\n",
    "    loudness_db = librosa.amplitude_to_db(loudness, ref=1.0)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(\n",
    "        y=audio, sr=sr, n_mfcc=30, hop_length=hop_length\n",
    "    ).T\n",
    "\n",
    "    min_len = min(len(f0_yin), len(loudness_db), len(mfcc))\n",
    "\n",
    "    return {\n",
    "        'f0': f0_yin[:min_len],\n",
    "        'loudness': loudness_db[:min_len],\n",
    "        'mfcc': mfcc[:min_len],\n",
    "        'audio': audio,\n",
    "        'n_frames': min_len\n",
    "    }\n",
    "\n",
    "\n",
    "def load_multiple_files(file_paths, sample_rate=22050, target_frames=None):\n",
    "    \"\"\"Load and batch multiple audio files for parallel training.\"\"\"\n",
    "    print(f\"üìä Loading {len(file_paths)} files...\")\n",
    "    all_features = []\n",
    "    \n",
    "    for path in tqdm(file_paths, desc=\"Extracting features\"):\n",
    "        features = extract_features(path, sample_rate)\n",
    "        all_features.append(features)\n",
    "        print(f\"   {Path(path).name}: F0 {features['f0'].min():.1f}-{features['f0'].max():.1f} Hz, {features['n_frames']} frames\")\n",
    "    \n",
    "    if target_frames is None:\n",
    "        target_frames = max(f['n_frames'] for f in all_features)\n",
    "    \n",
    "    print(f\"\\nüìê Padding all files to {target_frames} frames...\")\n",
    "    \n",
    "    batch_f0 = []\n",
    "    batch_loudness = []\n",
    "    batch_mfcc = []\n",
    "    batch_audio = []\n",
    "    \n",
    "    for features in all_features:\n",
    "        n = features['n_frames']\n",
    "        \n",
    "        if n < target_frames:\n",
    "            pad_f0 = np.pad(features['f0'], (0, target_frames - n), mode='edge')\n",
    "            pad_loud = np.pad(features['loudness'], (0, target_frames - n), mode='edge')\n",
    "            pad_mfcc = np.pad(features['mfcc'], ((0, target_frames - n), (0, 0)), mode='edge')\n",
    "            audio_samples = target_frames * 512\n",
    "            pad_audio = np.pad(features['audio'], (0, max(0, audio_samples - len(features['audio']))))\n",
    "        else:\n",
    "            pad_f0 = features['f0'][:target_frames]\n",
    "            pad_loud = features['loudness'][:target_frames]\n",
    "            pad_mfcc = features['mfcc'][:target_frames]\n",
    "            pad_audio = features['audio'][:target_frames * 512]\n",
    "        \n",
    "        batch_f0.append(pad_f0)\n",
    "        batch_loudness.append(pad_loud)\n",
    "        batch_mfcc.append(pad_mfcc)\n",
    "        batch_audio.append(pad_audio)\n",
    "    \n",
    "    print(f\"‚úÖ Batch prepared: {len(file_paths)} files √ó {target_frames} frames\")\n",
    "    \n",
    "    return {\n",
    "        'f0': torch.tensor(np.stack(batch_f0), dtype=torch.float32).unsqueeze(-1),\n",
    "        'loudness': torch.tensor(np.stack(batch_loudness), dtype=torch.float32).unsqueeze(-1),\n",
    "        'mfcc': torch.tensor(np.stack(batch_mfcc), dtype=torch.float32),\n",
    "        'audio': torch.tensor(np.stack(batch_audio), dtype=torch.float32),\n",
    "        'n_frames': target_frames,\n",
    "        'file_paths': file_paths\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"üìÅ Loading audio files...\\n\")\n",
    "batch_features = load_multiple_files(audio_files, sample_rate=22050)\n",
    "print(f\"\\nüéµ Ready to train on {len(audio_files)} scale tones simultaneously!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streamlined Training Loop (No torch.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batched tensors\n",
    "f0 = batch_features['f0'].to(device)\n",
    "loudness = batch_features['loudness'].to(device)\n",
    "mfcc = batch_features['mfcc'].to(device)\n",
    "target_audio = batch_features['audio'].to(device)\n",
    "\n",
    "print(f\"‚úÖ Batch loaded to {device}:\")\n",
    "print(f\"   f0: {f0.shape}\")\n",
    "print(f\"   loudness: {loudness.shape}\")\n",
    "print(f\"   mfcc: {mfcc.shape}\")\n",
    "print(f\"   audio: {target_audio.shape}\")\n",
    "\n",
    "# Create model (NO torch.compile)\n",
    "print(\"\\nüèóÔ∏è  Building model...\")\n",
    "model = DDSPModel(sample_rate=22050, n_harmonics=64, n_filter_banks=64, hidden_size=512).to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Parameters: {n_params:,}\")\n",
    "print(f\"   torch.compile: DISABLED (avoids hanging)\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Mixed Precision with correct API\n",
    "if DEVICE_TYPE:\n",
    "    scaler = GradScaler(DEVICE_TYPE)\n",
    "else:\n",
    "    scaler = GradScaler()\n",
    "print(\"‚ö° Mixed precision training enabled\")\n",
    "\n",
    "# Cached target STFT loss\n",
    "spectral_loss_fn = OptimizedMultiScaleSpectralLoss().to(device)\n",
    "spectral_loss_fn.precompute_target(target_audio, device)\n",
    "\n",
    "# Training configuration\n",
    "N_EPOCHS = 1000\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(f\"\\nüéØ Training {len(audio_files)} files for {N_EPOCHS} epochs...\")\n",
    "print(f\"   Expected time: ~60-90 minutes\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Test forward pass first\n",
    "print(\"üß™ Testing forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    if DEVICE_TYPE:\n",
    "        with autocast(device_type=DEVICE_TYPE):\n",
    "            test_out, _, _ = model(f0, loudness, mfcc)\n",
    "    else:\n",
    "        with autocast():\n",
    "            test_out, _, _ = model(f0, loudness, mfcc)\n",
    "print(f\"‚úÖ Forward pass successful! Output shape: {test_out.shape}\")\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(N_EPOCHS), desc=\"Training\"):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Mixed precision forward pass with correct API\n",
    "    if DEVICE_TYPE:\n",
    "        with autocast(device_type=DEVICE_TYPE):\n",
    "            pred_audio, harmonic_audio, noise_audio = model(f0, loudness, mfcc)\n",
    "            \n",
    "            min_len = min(pred_audio.shape[1], target_audio.shape[1])\n",
    "            pred_audio_trim = pred_audio[:, :min_len]\n",
    "            target_audio_trim = target_audio[:, :min_len]\n",
    "            \n",
    "            spec_loss = spectral_loss_fn(pred_audio_trim)\n",
    "            time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n",
    "            total_loss = 1.0 * spec_loss + 0.1 * time_loss\n",
    "    else:\n",
    "        with autocast():\n",
    "            pred_audio, harmonic_audio, noise_audio = model(f0, loudness, mfcc)\n",
    "            \n",
    "            min_len = min(pred_audio.shape[1], target_audio.shape[1])\n",
    "            pred_audio_trim = pred_audio[:, :min_len]\n",
    "            target_audio_trim = target_audio[:, :min_len]\n",
    "            \n",
    "            spec_loss = spectral_loss_fn(pred_audio_trim)\n",
    "            time_loss = F.l1_loss(pred_audio_trim, target_audio_trim)\n",
    "            total_loss = 1.0 * spec_loss + 0.1 * time_loss\n",
    "\n",
    "    # Backward pass\n",
    "    scaler.scale(total_loss).backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    loss_value = total_loss.item()\n",
    "    losses.append(loss_value)\n",
    "\n",
    "    if loss_value < best_loss:\n",
    "        best_loss = loss_value\n",
    "\n",
    "    # Show first epoch completion\n",
    "    if epoch == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚úÖ First epoch complete! ({elapsed:.1f}s)\")\n",
    "        print(f\"   Loss: {loss_value:.6f}\")\n",
    "        print(f\"   Training is working!\\n\")\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        eta = (elapsed / (epoch + 1)) * (N_EPOCHS - epoch - 1)\n",
    "        print(f\"\\nüìä Epoch {epoch+1}/{N_EPOCHS}:\")\n",
    "        print(f\"   Loss: {loss_value:.6f} | Best: {best_loss:.6f}\")\n",
    "        print(f\"   Time: {elapsed/60:.1f}min | ETA: {eta/60:.1f}min\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"   Per-file time: {total_time/60/len(audio_files):.1f} minutes\")\n",
    "print(f\"   Final loss: {best_loss:.6f}\")\n",
    "\n",
    "# Plot training curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses[-500:])\n",
    "plt.xlabel('Epoch (last 500)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (Detail)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
