{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timbre Generation: Conservative Fix ðŸŽ»\n",
    "\n",
    "**Lessons Learned**:\n",
    "- âŒ Circular padding broke temporal structure\n",
    "- âŒ Aggressive frequency weighting (5.0x) was too much\n",
    "- âœ… Original approach had structure, just needed gentle improvements\n",
    "\n",
    "**Conservative Changes**:\n",
    "1. âœ… Fix dB-to-power conversion: `/20 â†’ /10`\n",
    "2. âœ… Moderate frequency weighting: `2.0 â†’ 3.0` (not 5.0!)\n",
    "3. âœ… Moderate L1 weight: `0.1 â†’ 0.3` (not 0.5!)\n",
    "4. âœ… NO circular padding (keep original zero padding)\n",
    "\n",
    "**Expected**: Slightly better high-freq content, correct energy, minimal risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa soundfile bigvgan -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import soundfile as sf\n",
    "import os\n",
    "import bigvgan\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SAMPLE_RATE = 22050\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 256\n",
    "    N_MELS = 80\n",
    "    CELL_CHANNELS = 16\n",
    "    UPDATE_STEPS_TRAIN = 96\n",
    "    UPDATE_STEPS_INFERENCE = 96\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 8000\n",
    "    LOG_INTERVAL = 100\n",
    "    OUTPUT_IMAGE_DIR = 'training_progress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_auditory_grid(audio_path, config):\n",
    "    waveform, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE, mono=True)\n",
    "    n_frames = len(waveform) / config.HOP_LENGTH\n",
    "    if n_frames % 8 != 0:\n",
    "        target_frames = int(np.ceil(n_frames / 8.0)) * 8\n",
    "        target_samples = target_frames * config.HOP_LENGTH\n",
    "        padding_needed = target_samples - len(waveform)\n",
    "        waveform = np.pad(waveform, (0, padding_needed), 'constant')\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=waveform, sr=config.SAMPLE_RATE, n_fft=config.N_FFT,\n",
    "        hop_length=config.HOP_LENGTH, n_mels=config.N_MELS\n",
    "    )\n",
    "    return librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "def visualize_spectrogram(spectrogram, config, title='Mel-Spectrogram', output_path=None):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    if isinstance(spectrogram, torch.Tensor):\n",
    "        spectrogram = spectrogram.detach().cpu().numpy()\n",
    "    librosa.display.specshow(spectrogram, sr=config.SAMPLE_RATE, hop_length=config.HOP_LENGTH, \n",
    "                             x_axis='time', y_axis='mel')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading BigVGAN...\")\n",
    "bigvgan_model = bigvgan.BigVGAN.from_pretrained('nvidia/bigvgan_base_22khz_80band', use_cuda_kernel=False)\n",
    "bigvgan_model = bigvgan_model.to(device).eval()\n",
    "print(\"BigVGAN loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNCAModel(nn.Module):\n",
    "    \"\"\"Original TNCA (NO circular padding)\"\"\"\n",
    "    def __init__(self, num_channels=16):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        perception_vector_size = self.num_channels * 4\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Conv2d(perception_vector_size, 128, 1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, self.num_channels, 1, bias=True)\n",
    "        )\n",
    "        self.update_mlp[-1].weight.data.zero_()\n",
    "        self.update_mlp[-1].bias.data.zero_()\n",
    "        self.update_mlp[-1].bias.data[1] = 1.0\n",
    "\n",
    "    def perceive(self, grid):\n",
    "        device = grid.device\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
    "        sobel_y = sobel_x.transpose(-2, -1).contiguous()\n",
    "        laplacian = torch.tensor([[1, 2, 1], [2, -12, 2], [1, 2, 1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
    "        \n",
    "        sobel_x_kernel = sobel_x.repeat(self.num_channels, 1, 1, 1)\n",
    "        sobel_y_kernel = sobel_y.repeat(self.num_channels, 1, 1, 1)\n",
    "        laplacian_kernel = laplacian.repeat(self.num_channels, 1, 1, 1)\n",
    "        \n",
    "        # ORIGINAL zero padding (no circular!)\n",
    "        grad_x = F.conv2d(grid, sobel_x_kernel, padding=1, groups=self.num_channels)\n",
    "        grad_y = F.conv2d(grid, sobel_y_kernel, padding=1, groups=self.num_channels)\n",
    "        lap = F.conv2d(grid, laplacian_kernel, padding=1, groups=self.num_channels)\n",
    "        \n",
    "        return torch.cat([grid, grad_x, grad_y, lap], dim=1)\n",
    "\n",
    "    def forward(self, grid):\n",
    "        perception_vector = self.perceive(grid)\n",
    "        ds = self.update_mlp(perception_vector)\n",
    "        grid = grid + ds\n",
    "        clamped_alpha = torch.clamp(grid[:, 1:2, :, :], 0.0, 1.0)\n",
    "        grid = torch.cat([grid[:, :1, :, :], clamped_alpha, grid[:, 2:, :, :]], dim=1)\n",
    "        alpha_channel = grid[:, 1:2, :, :]\n",
    "        living_mask = F.max_pool2d(alpha_channel, kernel_size=3, stride=1, padding=1)\n",
    "        grid = grid * living_mask\n",
    "        return grid\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 5, 2, 2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU()\n",
    "        ).to(device)\n",
    "    \n",
    "    def gram_matrix(self, f):\n",
    "        b, c, h, w = f.size()\n",
    "        f = f.view(b, c, h * w)\n",
    "        return torch.bmm(f, f.transpose(1, 2)) / (c * h * w)\n",
    "    \n",
    "    def forward(self, gen_mel, target_mel):\n",
    "        return F.mse_loss(\n",
    "            self.gram_matrix(self.feature_extractor(gen_mel)),\n",
    "            self.gram_matrix(self.feature_extractor(target_mel))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, vocoder, loss_fn, optimizer, target_mel, config):\n",
    "    print(\"\\n--- Conservative Training ---\")\n",
    "    perceptual_loss_fn = loss_fn\n",
    "    l1_loss_fn = nn.L1Loss()\n",
    "    \n",
    "    target_mel_tensor = torch.tensor(target_mel, dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    target_mask = (target_mel_tensor > -70.0).float()\n",
    "    \n",
    "    # MODERATE frequency weighting: 1.0 â†’ 3.0 (not 5.0!)\n",
    "    n_mels = target_mel_tensor.shape[2]\n",
    "    frequency_loss_weights = torch.linspace(1.0, 3.0, n_mels, device=device).view(1, 1, n_mels, 1)\n",
    "    print(f\"Frequency weighting: 1.0 â†’ 3.0 (conservative)\")\n",
    "    \n",
    "    seed_grid = torch.zeros(1, config.CELL_CHANNELS, config.N_MELS, target_mel.shape[1], device=device)\n",
    "    h, w = seed_grid.shape[2], seed_grid.shape[3]\n",
    "    seed_grid[:, 1, h//2, w//2] = 1.0\n",
    "    \n",
    "    pbar = tqdm(range(config.NUM_EPOCHS), desc=\"Training...\")\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        grid = seed_grid.clone()\n",
    "        for _ in range(config.UPDATE_STEPS_TRAIN):\n",
    "            grid = model(grid)\n",
    "        \n",
    "        generated_mel_db_unscaled = grid[:, 0:1, :, :]\n",
    "        generated_mel_db = torch.tanh(generated_mel_db_unscaled) * 40.0 - 40.0\n",
    "        \n",
    "        p_loss = perceptual_loss_fn(generated_mel_db * target_mask, target_mel_tensor * target_mask)\n",
    "        l1_loss = l1_loss_fn(generated_mel_db * target_mask, target_mel_tensor * target_mask)\n",
    "        \n",
    "        # MODERATE L1 weight: 0.3 (not 0.5!)\n",
    "        loss = (2.0 * p_loss + 0.3 * l1_loss) * frequency_loss_weights\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % config.LOG_INTERVAL == 0:\n",
    "            pbar.set_description(f\"Epoch {epoch+1}, Loss: {loss.item():.6f} (P: {p_loss.item():.6f}, L1: {l1_loss.item():.6f})\")\n",
    "            filepath = os.path.join(config.OUTPUT_IMAGE_DIR, f'epoch_{(epoch+1):04d}.png')\n",
    "            visualize_spectrogram(generated_mel_db.squeeze(), config, \n",
    "                                title=f'Conservative - Epoch {epoch+1}', output_path=filepath)\n",
    "    \n",
    "    print(f\"\\nTraining complete. Final loss: {loss.item():.6f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, vocoder, target_mel, config, output_path='generated_conservative.wav'):\n",
    "    print(\"\\n--- Conservative Inference ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    if isinstance(target_mel, np.ndarray):\n",
    "        target_mel = torch.tensor(target_mel, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        target_mel = target_mel.to(device)\n",
    "    \n",
    "    seed_grid = torch.zeros(1, config.CELL_CHANNELS, config.N_MELS, target_mel.shape[1], device=device)\n",
    "    h, w = seed_grid.shape[2], seed_grid.shape[3]\n",
    "    seed_grid[:, 1, h//2, w//2] = 1.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        final_grid = seed_grid.clone()\n",
    "        for _ in tqdm(range(config.UPDATE_STEPS_INFERENCE), desc=\"Generating...\"):\n",
    "            final_grid = model(final_grid)\n",
    "        \n",
    "        final_mel_db_unscaled = final_grid[:, 0, :, :]\n",
    "        final_mel_db = torch.tanh(final_mel_db_unscaled) * 40.0 - 40.0\n",
    "        \n",
    "        # FIXED: dB to power (10^(dB/10))\n",
    "        print(\"Converting dB to power: 10^(dB/10)\")\n",
    "        final_mel_power = torch.pow(10.0, final_mel_db / 10.0)\n",
    "        \n",
    "        if final_mel_power.dim() == 2:\n",
    "            final_mel_power = final_mel_power.unsqueeze(0)\n",
    "        \n",
    "        print(\"Generating audio with BigVGAN...\")\n",
    "        final_waveform = vocoder(final_mel_power).squeeze()\n",
    "    \n",
    "    waveform_np = final_waveform.cpu().numpy()\n",
    "    sf.write(output_path, waveform_np, config.SAMPLE_RATE)\n",
    "    \n",
    "    print(f\"\\nâœ… Audio saved: {output_path}\")\n",
    "    print(f\"Duration: {len(waveform_np) / config.SAMPLE_RATE:.2f}s\")\n",
    "    print(f\"RMS energy: {np.sqrt(np.mean(waveform_np**2)):.4f}\")\n",
    "    \n",
    "    visualize_spectrogram(final_mel_db.squeeze().cpu(), config, title='Final (Conservative)')\n",
    "    return waveform_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "target_audio_path = 'violin.wav'\n",
    "\n",
    "if not os.path.exists(target_audio_path):\n",
    "    print(f\"ERROR: {target_audio_path} not found\")\n",
    "else:\n",
    "    target_spectrogram = create_auditory_grid(target_audio_path, config)\n",
    "    visualize_spectrogram(target_spectrogram, config, title='Target')\n",
    "    \n",
    "    tnca_model = TNCAModel(config.CELL_CHANNELS).to(device)\n",
    "    perceptual_loss_fn = PerceptualLoss(device).to(device)\n",
    "    optimizer = optim.Adam(tnca_model.parameters(), lr=config.LEARNING_RATE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONSERVATIVE APPROACH:\")\n",
    "    print(\"  âœ… NO circular padding (original zero padding)\")\n",
    "    print(\"  âœ… Frequency weighting: 1.0 â†’ 3.0 (moderate)\")\n",
    "    print(\"  âœ… L1 weight: 0.3 (moderate increase)\")\n",
    "    print(\"  âœ… dB conversion: 10^(dB/10) for power\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    trained_model = train(tnca_model, bigvgan_model, perceptual_loss_fn, optimizer, target_spectrogram, config)\n",
    "    generated_audio = inference(trained_model, bigvgan_model, target_spectrogram, config)\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Conservative approach complete!\")\n",
    "    print(\"Expected: Stable training, slight high-freq improvement, correct energy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
