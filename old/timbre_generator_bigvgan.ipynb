{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Audio Timbres with Neural Cellular Automata ðŸŽ¶\n",
    "## Phase 1: BigVGAN Integration\n",
    "\n",
    "This notebook uses **BigVGAN** universal vocoder instead of speech-trained HiFi-GAN.\n",
    "\n",
    "**Key Improvement**: BigVGAN is trained on diverse audio (not just speech), providing appropriate phase reconstruction for musical timbres.\n",
    "\n",
    "**Workflow:**\n",
    "1. **Setup**: Install BigVGAN and other necessary libraries\n",
    "2. **Configuration**: Audio processing parameters (22kHz, 80 mel bins)\n",
    "3. **Load Data**: Convert target audio to mel spectrogram\n",
    "4. **Train TNCA**: Neural cellular automata learns to grow the spectrogram\n",
    "5. **Inference**: Generate audio with BigVGAN vocoder (should sound like violin!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install librosa soundfile\n",
    "!pip install bigvgan  # Universal vocoder for music/speech\n",
    "\n",
    "# Import all required modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import soundfile as sf\n",
    "import os\n",
    "import bigvgan\n",
    "\n",
    "# Device configuration\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SAMPLE_RATE = 22050\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 256\n",
    "    N_MELS = 80  # Matches BigVGAN base model (80 mel bands)\n",
    "    CELL_CHANNELS = 16\n",
    "    UPDATE_STEPS_TRAIN = 96\n",
    "    UPDATE_STEPS_INFERENCE = 96\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 8000\n",
    "    LOG_INTERVAL = 100\n",
    "    OUTPUT_IMAGE_DIR = 'training_progress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_auditory_grid(audio_path, config):\n",
    "    \"\"\"Loads audio and converts to mel spectrogram.\"\"\"\n",
    "    waveform, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE, mono=True)\n",
    "\n",
    "    # Ensure time dimension is multiple of 8 for BigVGAN\n",
    "    n_frames = len(waveform) / config.HOP_LENGTH\n",
    "    if n_frames % 8 != 0:\n",
    "        target_frames = int(np.ceil(n_frames / 8.0)) * 8\n",
    "        target_samples = target_frames * config.HOP_LENGTH\n",
    "        padding_needed = target_samples - len(waveform)\n",
    "        waveform = np.pad(waveform, (0, padding_needed), 'constant')\n",
    "\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=waveform, \n",
    "        sr=config.SAMPLE_RATE, \n",
    "        n_fft=config.N_FFT,\n",
    "        hop_length=config.HOP_LENGTH, \n",
    "        n_mels=config.N_MELS\n",
    "    )\n",
    "    return librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "def visualize_spectrogram(spectrogram, config, title='Mel-Spectrogram', output_path=None):\n",
    "    \"\"\"Visualizes a mel spectrogram.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    if isinstance(spectrogram, torch.Tensor):\n",
    "        spectrogram = spectrogram.detach().cpu().numpy()\n",
    "\n",
    "    librosa.display.specshow(\n",
    "        spectrogram, \n",
    "        sr=config.SAMPLE_RATE, \n",
    "        hop_length=config.HOP_LENGTH, \n",
    "        x_axis='time', \n",
    "        y_axis='mel'\n",
    "    )\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BigVGAN universal vocoder\n",
    "print(\"Loading BigVGAN universal vocoder...\")\n",
    "bigvgan_model = bigvgan.BigVGAN.from_pretrained(\n",
    "    'nvidia/bigvgan_base_22khz_80band',  # Matches our 22kHz sample rate and 80 mel bins\n",
    "    use_cuda_kernel=False  # Set to True if you have matching CUDA version\n",
    ")\n",
    "bigvgan_model = bigvgan_model.to(device)\n",
    "bigvgan_model.eval()\n",
    "print(\"BigVGAN vocoder loaded successfully!\")\n",
    "print(f\"Sample rate: 22kHz, Mel bands: 80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNCAModel(nn.Module):\n",
    "    \"\"\"Textured Neural Cellular Automata for spectrogram generation.\"\"\"\n",
    "    def __init__(self, num_channels=16):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        perception_vector_size = self.num_channels * 4\n",
    "\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Conv2d(perception_vector_size, 128, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, self.num_channels, 1, bias=True)\n",
    "        )\n",
    "        \n",
    "        # Initialize for stability\n",
    "        self.update_mlp[-1].weight.data.zero_()\n",
    "        self.update_mlp[-1].bias.data.zero_()\n",
    "        self.update_mlp[-1].bias.data[1] = 1.0  # Alpha channel bias\n",
    "\n",
    "    def perceive(self, grid):\n",
    "        \"\"\"Apply perception filters (Sobel, Laplacian).\"\"\"\n",
    "        device = grid.device\n",
    "        \n",
    "        sobel_x = torch.tensor(\n",
    "            [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "            dtype=torch.float32, device=device\n",
    "        ).view(1, 1, 3, 3)\n",
    "        sobel_y = sobel_x.transpose(-2, -1).contiguous()\n",
    "        laplacian = torch.tensor(\n",
    "            [[1, 2, 1], [2, -12, 2], [1, 2, 1]], \n",
    "            dtype=torch.float32, device=device\n",
    "        ).view(1, 1, 3, 3)\n",
    "\n",
    "        # Create kernels for each channel\n",
    "        sobel_x_kernel = sobel_x.repeat(self.num_channels, 1, 1, 1)\n",
    "        sobel_y_kernel = sobel_y.repeat(self.num_channels, 1, 1, 1)\n",
    "        laplacian_kernel = laplacian.repeat(self.num_channels, 1, 1, 1)\n",
    "\n",
    "        # Apply filters\n",
    "        grad_x = F.conv2d(grid, sobel_x_kernel, padding=1, groups=self.num_channels)\n",
    "        grad_y = F.conv2d(grid, sobel_y_kernel, padding=1, groups=self.num_channels)\n",
    "        lap = F.conv2d(grid, laplacian_kernel, padding=1, groups=self.num_channels)\n",
    "\n",
    "        return torch.cat([grid, grad_x, grad_y, lap], dim=1)\n",
    "\n",
    "    def forward(self, grid):\n",
    "        perception_vector = self.perceive(grid)\n",
    "        ds = self.update_mlp(perception_vector)\n",
    "        grid = grid + ds\n",
    "\n",
    "        # Clamp alpha channel\n",
    "        clamped_alpha = torch.clamp(grid[:, 1:2, :, :], 0.0, 1.0)\n",
    "        grid = torch.cat([grid[:, :1, :, :], clamped_alpha, grid[:, 2:, :, :]], dim=1)\n",
    "\n",
    "        # Apply living mask\n",
    "        alpha_channel = grid[:, 1:2, :, :]\n",
    "        living_mask = F.max_pool2d(alpha_channel, kernel_size=3, stride=1, padding=1)\n",
    "        grid = grid * living_mask\n",
    "\n",
    "        return grid\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"Perceptual loss based on Gram matrix.\"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 5, 2, 2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU()\n",
    "        ).to(device)\n",
    "\n",
    "    def gram_matrix(self, f):\n",
    "        b, c, h, w = f.size()\n",
    "        f = f.view(b, c, h * w)\n",
    "        return torch.bmm(f, f.transpose(1, 2)) / (c * h * w)\n",
    "\n",
    "    def forward(self, gen_mel, target_mel):\n",
    "        return F.mse_loss(\n",
    "            self.gram_matrix(self.feature_extractor(gen_mel)),\n",
    "            self.gram_matrix(self.feature_extractor(target_mel))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, vocoder, loss_fn, optimizer, target_mel, config):\n",
    "    \"\"\"Train the TNCA model.\"\"\"\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    perceptual_loss_fn = loss_fn\n",
    "    l1_loss_fn = nn.L1Loss()\n",
    "\n",
    "    target_mel_tensor = torch.tensor(\n",
    "        target_mel, dtype=torch.float32\n",
    "    ).to(device).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "    # Mask for non-silent regions\n",
    "    target_mask = (target_mel_tensor > -70.0).float()\n",
    "\n",
    "    # Frequency weighting (higher frequencies weighted more)\n",
    "    n_mels = target_mel_tensor.shape[2]\n",
    "    frequency_loss_weights = torch.linspace(\n",
    "        1.0, 2.0, n_mels, device=device\n",
    "    ).view(1, 1, n_mels, 1)\n",
    "\n",
    "    # Initialize seed grid\n",
    "    seed_grid = torch.zeros(\n",
    "        1, config.CELL_CHANNELS, config.N_MELS, target_mel.shape[1], device=device\n",
    "    )\n",
    "    h, w = seed_grid.shape[2], seed_grid.shape[3]\n",
    "    seed_grid[:, 1, h//2, w//2] = 1.0  # Single living cell at center\n",
    "\n",
    "    pbar = tqdm(range(config.NUM_EPOCHS), desc=\"Training...\")\n",
    "\n",
    "    for epoch in pbar:\n",
    "        grid = seed_grid.clone()\n",
    "        \n",
    "        # Evolve the grid\n",
    "        for _ in range(config.UPDATE_STEPS_TRAIN):\n",
    "            grid = model(grid)\n",
    "\n",
    "        # Extract and scale to dB range\n",
    "        generated_mel_db_unscaled = grid[:, 0:1, :, :]\n",
    "        generated_mel_db = torch.tanh(generated_mel_db_unscaled) * 40.0 - 40.0\n",
    "\n",
    "        # Calculate losses\n",
    "        p_loss = perceptual_loss_fn(\n",
    "            generated_mel_db * target_mask, \n",
    "            target_mel_tensor * target_mask\n",
    "        )\n",
    "        l1_loss = l1_loss_fn(\n",
    "            generated_mel_db * target_mask, \n",
    "            target_mel_tensor * target_mask\n",
    "        )\n",
    "        \n",
    "        loss = (2.0 * p_loss + 0.1 * l1_loss) * frequency_loss_weights\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if (epoch + 1) % config.LOG_INTERVAL == 0:\n",
    "            pbar.set_description(\n",
    "                f\"Epoch {epoch+1}, Loss: {loss.item():.6f} \"\n",
    "                f\"(P: {p_loss.item():.6f}, L1: {l1_loss.item():.6f})\"\n",
    "            )\n",
    "            filepath = os.path.join(\n",
    "                config.OUTPUT_IMAGE_DIR, \n",
    "                f'epoch_{(epoch+1):04d}.png'\n",
    "            )\n",
    "            visualize_spectrogram(\n",
    "                generated_mel_db.squeeze(), \n",
    "                config,\n",
    "                title=f'Generated Spectrogram - Epoch {epoch+1}',\n",
    "                output_path=filepath\n",
    "            )\n",
    "\n",
    "    print(f\"\\nTraining complete. Final loss: {loss.item():.6f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, vocoder, target_mel, config, output_path='generated_bigvgan.wav'):\n",
    "    \"\"\"Generate audio using trained TNCA and BigVGAN vocoder.\"\"\"\n",
    "    print(\"\\n--- Running Inference with BigVGAN ---\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure target_mel is tensor on device\n",
    "    if isinstance(target_mel, np.ndarray):\n",
    "        target_mel = torch.tensor(target_mel, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        target_mel = target_mel.to(device)\n",
    "\n",
    "    # Initialize seed grid\n",
    "    seed_grid = torch.zeros(\n",
    "        1, config.CELL_CHANNELS, config.N_MELS, target_mel.shape[1], device=device\n",
    "    )\n",
    "    h, w = seed_grid.shape[2], seed_grid.shape[3]\n",
    "    seed_grid[:, 1, h//2, w//2] = 1.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        final_grid = seed_grid.clone()\n",
    "        \n",
    "        for _ in tqdm(range(config.UPDATE_STEPS_INFERENCE), desc=\"Generating...\"):\n",
    "            final_grid = model(final_grid)\n",
    "\n",
    "        # Extract and scale\n",
    "        final_mel_db_unscaled = final_grid[:, 0, :, :]\n",
    "        final_mel_db = torch.tanh(final_mel_db_unscaled) * 40.0 - 40.0\n",
    "\n",
    "        # Convert dB to linear scale for BigVGAN\n",
    "        # BigVGAN expects mel spectrogram in linear scale, not dB\n",
    "        final_mel_linear = torch.pow(10.0, final_mel_db / 20.0)  # dB to amplitude\n",
    "        \n",
    "        print(f\"Mel spectrogram shape: {final_mel_linear.shape}\")\n",
    "        print(f\"Mel spectrogram range: [{final_mel_linear.min():.4f}, {final_mel_linear.max():.4f}]\")\n",
    "\n",
    "        # Add batch dimension if needed: BigVGAN expects [batch, mels, time]\n",
    "        if final_mel_linear.dim() == 2:\n",
    "            final_mel_linear = final_mel_linear.unsqueeze(0)\n",
    "\n",
    "        print(\"Generating audio with BigVGAN vocoder...\")\n",
    "        \n",
    "        # BigVGAN inference\n",
    "        final_waveform = vocoder(final_mel_linear)\n",
    "        \n",
    "        # BigVGAN returns [batch, 1, samples], squeeze to [samples]\n",
    "        final_waveform = final_waveform.squeeze()\n",
    "\n",
    "    # Save audio\n",
    "    waveform_np = final_waveform.cpu().numpy()\n",
    "    sf.write(output_path, waveform_np, config.SAMPLE_RATE)\n",
    "    \n",
    "    print(f\"\\nInference complete!\")\n",
    "    print(f\"Audio saved to: {output_path}\")\n",
    "    print(f\"Audio duration: {len(waveform_np) / config.SAMPLE_RATE:.2f} seconds\")\n",
    "    \n",
    "    visualize_spectrogram(\n",
    "        final_mel_db.squeeze().cpu(), \n",
    "        config, \n",
    "        title='Final Generated Spectrogram (BigVGAN)'\n",
    "    )\n",
    "    \n",
    "    return waveform_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Load target audio\n",
    "target_audio_path = 'violin.wav'  # <-- Update with your file path\n",
    "\n",
    "if not os.path.exists(target_audio_path):\n",
    "    print(f\"ERROR: Audio file not found at '{target_audio_path}'\")\n",
    "    print(\"Please upload the file and update the path.\")\n",
    "else:\n",
    "    print(\"Loading target audio...\")\n",
    "    target_spectrogram = create_auditory_grid(target_audio_path, config)\n",
    "    \n",
    "    print(\"\\nTarget Spectrogram:\")\n",
    "    visualize_spectrogram(target_spectrogram, config, title='Target Spectrogram')\n",
    "\n",
    "    # Initialize models\n",
    "    print(\"\\nInitializing TNCA model...\")\n",
    "    tnca_model = TNCAModel(config.CELL_CHANNELS).to(device)\n",
    "    perceptual_loss_fn = PerceptualLoss(device).to(device)\n",
    "    optimizer = optim.Adam(tnca_model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    # Train\n",
    "    trained_model = train(\n",
    "        tnca_model, \n",
    "        bigvgan_model, \n",
    "        perceptual_loss_fn, \n",
    "        optimizer, \n",
    "        target_spectrogram, \n",
    "        config\n",
    "    )\n",
    "\n",
    "    # Generate audio with BigVGAN\n",
    "    generated_audio = inference(\n",
    "        trained_model, \n",
    "        bigvgan_model, \n",
    "        target_spectrogram, \n",
    "        config, \n",
    "        output_path='generated_bigvgan.wav'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ‰ PHASE 1 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nListen to 'generated_bigvgan.wav' and compare with original violin.\")\n",
    "    print(\"Expected: Violin-like timbre (not noise!)\")\n",
    "    print(\"\\nIf quality is good, proceed to Phase 2: Audio-domain loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
