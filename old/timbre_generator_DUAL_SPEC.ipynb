{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timbre Generation: Dual-Spectrogram Approach ðŸŽ¼\n",
    "\n",
    "**Innovation**: Generate BOTH mel + linear spectrograms simultaneously\n",
    "\n",
    "**Why Dual Representations**:\n",
    "- **Mel (80 bands)**: Perceptually relevant, matches human hearing, good for vocoder\n",
    "- **Linear (513 bands)**: Full frequency resolution, captures all harmonics, preserves detail\n",
    "- **Complementary**: Each compensates for the other's weaknesses\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "TNCA Output Channels:\n",
    "â”œâ”€ Channel 0: Mel spectrogram value\n",
    "â”œâ”€ Channel 1: Alpha (life mask)\n",
    "â”œâ”€ Channel 2: Linear spectrogram value\n",
    "â””â”€ Channels 3-15: Hidden state\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa soundfile bigvgan -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import soundfile as sf\n",
    "import os\n",
    "import bigvgan\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SAMPLE_RATE = 22050\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 256\n",
    "    N_MELS = 80  # Mel spectrogram bands\n",
    "    N_LINEAR = 513  # Linear spectrogram bands (N_FFT//2 + 1)\n",
    "    CELL_CHANNELS = 16  # Total channels in TNCA\n",
    "    UPDATE_STEPS_TRAIN = 96\n",
    "    UPDATE_STEPS_INFERENCE = 96\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 8000\n",
    "    LOG_INTERVAL = 100\n",
    "    OUTPUT_IMAGE_DIR = 'training_progress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dual_spectrograms(audio_path, config):\n",
    "    \"\"\"Create BOTH mel and linear spectrograms.\"\"\"\n",
    "    waveform, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE, mono=True)\n",
    "    \n",
    "    # Padding\n",
    "    n_frames = len(waveform) / config.HOP_LENGTH\n",
    "    if n_frames % 8 != 0:\n",
    "        target_frames = int(np.ceil(n_frames / 8.0)) * 8\n",
    "        target_samples = target_frames * config.HOP_LENGTH\n",
    "        padding_needed = target_samples - len(waveform)\n",
    "        waveform = np.pad(waveform, (0, padding_needed), 'constant')\n",
    "    \n",
    "    # Mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=waveform, sr=config.SAMPLE_RATE, n_fft=config.N_FFT,\n",
    "        hop_length=config.HOP_LENGTH, n_mels=config.N_MELS\n",
    "    )\n",
    "    mel_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Linear spectrogram (full frequency resolution)\n",
    "    linear_spec = np.abs(librosa.stft(waveform, n_fft=config.N_FFT, hop_length=config.HOP_LENGTH))**2\n",
    "    linear_db = librosa.power_to_db(linear_spec, ref=np.max)\n",
    "    \n",
    "    return mel_db, linear_db, waveform\n",
    "\n",
    "def visualize_dual_spectrograms(mel_spec, linear_spec, config, title_prefix='', output_path=None):\n",
    "    \"\"\"Visualize both spectrograms side by side.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "    \n",
    "    if isinstance(mel_spec, torch.Tensor):\n",
    "        mel_spec = mel_spec.detach().cpu().numpy()\n",
    "    if isinstance(linear_spec, torch.Tensor):\n",
    "        linear_spec = linear_spec.detach().cpu().numpy()\n",
    "    \n",
    "    # Mel spectrogram\n",
    "    librosa.display.specshow(mel_spec, sr=config.SAMPLE_RATE, hop_length=config.HOP_LENGTH,\n",
    "                             x_axis='time', y_axis='mel', ax=axes[0])\n",
    "    axes[0].set_title(f'{title_prefix} Mel Spectrogram (80 bands)')\n",
    "    plt.colorbar(axes[0].collections[0], ax=axes[0], format='%+2.0f dB')\n",
    "    \n",
    "    # Linear spectrogram\n",
    "    librosa.display.specshow(linear_spec, sr=config.SAMPLE_RATE, hop_length=config.HOP_LENGTH,\n",
    "                             x_axis='time', y_axis='linear', ax=axes[1])\n",
    "    axes[1].set_title(f'{title_prefix} Linear Spectrogram (513 bands)')\n",
    "    plt.colorbar(axes[1].collections[0], ax=axes[1], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading BigVGAN...\")\n",
    "bigvgan_model = bigvgan.BigVGAN.from_pretrained('nvidia/bigvgan_base_22khz_80band', use_cuda_kernel=False)\n",
    "bigvgan_model = bigvgan_model.to(device).eval()\n",
    "print(\"BigVGAN loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualSpecTNCAModel(nn.Module):\n",
    "    \"\"\"TNCA that generates BOTH mel and linear spectrograms.\"\"\"\n",
    "    def __init__(self, num_channels=16, n_mels=80, n_linear=513):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.n_mels = n_mels\n",
    "        self.n_linear = n_linear\n",
    "        \n",
    "        perception_vector_size = self.num_channels * 4\n",
    "        \n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Conv2d(perception_vector_size, 128, 1), nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, 1), nn.ReLU(),\n",
    "            nn.Conv2d(64, self.num_channels, 1, bias=True)\n",
    "        )\n",
    "        \n",
    "        self.update_mlp[-1].weight.data.zero_()\n",
    "        self.update_mlp[-1].bias.data.zero_()\n",
    "        self.update_mlp[-1].bias.data[1] = 1.0  # Alpha channel\n",
    "    \n",
    "    def perceive(self, grid):\n",
    "        device = grid.device\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
    "        sobel_y = sobel_x.transpose(-2, -1).contiguous()\n",
    "        laplacian = torch.tensor([[1, 2, 1], [2, -12, 2], [1, 2, 1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
    "        \n",
    "        sobel_x_kernel = sobel_x.repeat(self.num_channels, 1, 1, 1)\n",
    "        sobel_y_kernel = sobel_y.repeat(self.num_channels, 1, 1, 1)\n",
    "        laplacian_kernel = laplacian.repeat(self.num_channels, 1, 1, 1)\n",
    "        \n",
    "        grad_x = F.conv2d(grid, sobel_x_kernel, padding=1, groups=self.num_channels)\n",
    "        grad_y = F.conv2d(grid, sobel_y_kernel, padding=1, groups=self.num_channels)\n",
    "        lap = F.conv2d(grid, laplacian_kernel, padding=1, groups=self.num_channels)\n",
    "        \n",
    "        return torch.cat([grid, grad_x, grad_y, lap], dim=1)\n",
    "    \n",
    "    def forward(self, grid_mel, grid_linear):\n",
    "        \"\"\"Forward pass for BOTH grids.\"\"\"\n",
    "        # Process mel grid\n",
    "        perception_mel = self.perceive(grid_mel)\n",
    "        ds_mel = self.update_mlp(perception_mel)\n",
    "        grid_mel = grid_mel + ds_mel\n",
    "        \n",
    "        # Process linear grid\n",
    "        perception_linear = self.perceive(grid_linear)\n",
    "        ds_linear = self.update_mlp(perception_linear)\n",
    "        grid_linear = grid_linear + ds_linear\n",
    "        \n",
    "        # Apply alpha clamping and living mask to both\n",
    "        for grid in [grid_mel, grid_linear]:\n",
    "            clamped_alpha = torch.clamp(grid[:, 1:2, :, :], 0.0, 1.0)\n",
    "            grid = torch.cat([grid[:, :1, :, :], clamped_alpha, grid[:, 2:, :, :]], dim=1)\n",
    "            alpha_channel = grid[:, 1:2, :, :]\n",
    "            living_mask = F.max_pool2d(alpha_channel, kernel_size=3, stride=1, padding=1)\n",
    "            grid = grid * living_mask\n",
    "        \n",
    "        return grid_mel, grid_linear\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 5, 2, 2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU()\n",
    "        ).to(device)\n",
    "    \n",
    "    def gram_matrix(self, f):\n",
    "        b, c, h, w = f.size()\n",
    "        f = f.view(b, c, h * w)\n",
    "        return torch.bmm(f, f.transpose(1, 2)) / (c * h * w)\n",
    "    \n",
    "    def forward(self, gen_mel, target_mel):\n",
    "        return F.mse_loss(\n",
    "            self.gram_matrix(self.feature_extractor(gen_mel)),\n",
    "            self.gram_matrix(self.feature_extractor(target_mel))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dual(model, vocoder, loss_fn, optimizer, target_mel, target_linear, config):\n",
    "    print(\"\\n--- Dual-Spectrogram Training ---\")\n",
    "    perceptual_loss_fn = loss_fn\n",
    "    l1_loss_fn = nn.L1Loss()\n",
    "    \n",
    "    # Prepare targets\n",
    "    target_mel_tensor = torch.tensor(target_mel, dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    target_linear_tensor = torch.tensor(target_linear, dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    \n",
    "    target_mel_mask = (target_mel_tensor > -70.0).float()\n",
    "    target_linear_mask = (target_linear_tensor > -70.0).float()\n",
    "    \n",
    "    # Initialize seed grids (DIFFERENT sizes!)\n",
    "    seed_grid_mel = torch.zeros(1, config.CELL_CHANNELS, config.N_MELS, target_mel.shape[1], device=device)\n",
    "    seed_grid_linear = torch.zeros(1, config.CELL_CHANNELS, config.N_LINEAR, target_linear.shape[1], device=device)\n",
    "    \n",
    "    # Seed centers\n",
    "    seed_grid_mel[:, 1, config.N_MELS//2, target_mel.shape[1]//2] = 1.0\n",
    "    seed_grid_linear[:, 1, config.N_LINEAR//2, target_linear.shape[1]//2] = 1.0\n",
    "    \n",
    "    pbar = tqdm(range(config.NUM_EPOCHS), desc=\"Training...\")\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        grid_mel = seed_grid_mel.clone()\n",
    "        grid_linear = seed_grid_linear.clone()\n",
    "        \n",
    "        # Evolve BOTH grids\n",
    "        for _ in range(config.UPDATE_STEPS_TRAIN):\n",
    "            grid_mel, grid_linear = model(grid_mel, grid_linear)\n",
    "        \n",
    "        # Extract values\n",
    "        gen_mel_db = torch.tanh(grid_mel[:, 0:1, :, :]) * 40.0 - 40.0\n",
    "        gen_linear_db = torch.tanh(grid_linear[:, 0:1, :, :]) * 40.0 - 40.0\n",
    "        \n",
    "        # Dual loss\n",
    "        p_loss_mel = perceptual_loss_fn(gen_mel_db * target_mel_mask, target_mel_tensor * target_mel_mask)\n",
    "        l1_loss_mel = l1_loss_fn(gen_mel_db * target_mel_mask, target_mel_tensor * target_mel_mask)\n",
    "        mel_loss = 2.0 * p_loss_mel + 0.3 * l1_loss_mel\n",
    "        \n",
    "        l1_loss_linear = l1_loss_fn(gen_linear_db * target_linear_mask, target_linear_tensor * target_linear_mask)\n",
    "        linear_loss = l1_loss_linear  # Linear uses L1 only (no perceptual)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = mel_loss + 0.5 * linear_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % config.LOG_INTERVAL == 0:\n",
    "            pbar.set_description(\n",
    "                f\"Epoch {epoch+1}, Loss: {total_loss.item():.6f} \"\n",
    "                f\"(Mel: {mel_loss.item():.6f}, Lin: {linear_loss.item():.6f})\"\n",
    "            )\n",
    "            filepath = os.path.join(config.OUTPUT_IMAGE_DIR, f'epoch_{(epoch+1):04d}.png')\n",
    "            visualize_dual_spectrograms(\n",
    "                gen_mel_db.squeeze(), gen_linear_db.squeeze(), config,\n",
    "                title_prefix=f'Epoch {epoch+1}', output_path=filepath\n",
    "            )\n",
    "    \n",
    "    print(f\"\\nTraining complete. Final loss: {total_loss.item():.6f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_dual(model, vocoder, target_mel, target_linear, config, output_path='generated_dual.wav'):\n",
    "    print(\"\\n--- Dual-Spectrogram Inference ---\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize seed grids\n",
    "    seed_grid_mel = torch.zeros(1, config.CELL_CHANNELS, config.N_MELS, target_mel.shape[1], device=device)\n",
    "    seed_grid_linear = torch.zeros(1, config.CELL_CHANNELS, config.N_LINEAR, target_linear.shape[1], device=device)\n",
    "    seed_grid_mel[:, 1, config.N_MELS//2, target_mel.shape[1]//2] = 1.0\n",
    "    seed_grid_linear[:, 1, config.N_LINEAR//2, target_linear.shape[1]//2] = 1.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        grid_mel = seed_grid_mel.clone()\n",
    "        grid_linear = seed_grid_linear.clone()\n",
    "        \n",
    "        for _ in tqdm(range(config.UPDATE_STEPS_INFERENCE), desc=\"Generating...\"):\n",
    "            grid_mel, grid_linear = model(grid_mel, grid_linear)\n",
    "        \n",
    "        final_mel_db = torch.tanh(grid_mel[:, 0, :, :]) * 40.0 - 40.0\n",
    "        final_linear_db = torch.tanh(grid_linear[:, 0, :, :]) * 40.0 - 40.0\n",
    "        \n",
    "        # Convert to power\n",
    "        final_mel_power = torch.pow(10.0, final_mel_db / 10.0)\n",
    "        \n",
    "        if final_mel_power.dim() == 2:\n",
    "            final_mel_power = final_mel_power.unsqueeze(0)\n",
    "        \n",
    "        print(\"Using MEL spectrogram for BigVGAN vocoding...\")\n",
    "        final_waveform = vocoder(final_mel_power).squeeze()\n",
    "    \n",
    "    waveform_np = final_waveform.cpu().numpy()\n",
    "    sf.write(output_path, waveform_np, config.SAMPLE_RATE)\n",
    "    \n",
    "    print(f\"\\nâœ… Audio saved: {output_path}\")\n",
    "    print(f\"RMS energy: {np.sqrt(np.mean(waveform_np**2)):.4f}\")\n",
    "    \n",
    "    visualize_dual_spectrograms(final_mel_db.squeeze().cpu(), final_linear_db.squeeze().cpu(), \n",
    "                               config, title_prefix='Final')\n",
    "    return waveform_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "target_audio_path = 'violin.wav'\n",
    "\n",
    "if not os.path.exists(target_audio_path):\n",
    "    print(f\"ERROR: {target_audio_path} not found\")\n",
    "else:\n",
    "    target_mel, target_linear, waveform = create_dual_spectrograms(target_audio_path, config)\n",
    "    visualize_dual_spectrograms(target_mel, target_linear, config, title_prefix='Target')\n",
    "    \n",
    "    model = DualSpecTNCAModel(config.CELL_CHANNELS, config.N_MELS, config.N_LINEAR).to(device)\n",
    "    perceptual_loss_fn = PerceptualLoss(device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DUAL-SPECTROGRAM APPROACH:\")\n",
    "    print(f\"  âœ… Mel: {config.N_MELS} bands (perceptual)\")\n",
    "    print(f\"  âœ… Linear: {config.N_LINEAR} bands (full resolution)\")\n",
    "    print(\"  âœ… Complementary representations\")\n",
    "    print(\"  âœ… Better high-frequency capture expected\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    trained_model = train_dual(model, bigvgan_model, perceptual_loss_fn, optimizer, \n",
    "                               target_mel, target_linear, config)\n",
    "    generated_audio = inference_dual(trained_model, bigvgan_model, target_mel, target_linear, config)\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Dual-spectrogram training complete!\")\n",
    "    print(\"Linear spectrogram should capture high-frequency detail missed by mel-only\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
