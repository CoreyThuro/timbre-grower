{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Audio Timbres with Neural Cellular Automata ðŸŽ¶\n",
    "## Phase 1: BigVGAN Integration (FIXED VERSION)\n",
    "\n",
    "**Fixes Applied**:\n",
    "1. âœ… Corrected dB-to-power conversion (was `/20`, now `/10`)\n",
    "2. âœ… Increased frequency weighting (from `2.0` to `5.0`)\n",
    "3. âœ… Increased L1 loss weight (from `0.1` to `0.5`)\n",
    "4. âœ… Added circular padding for CA boundary conditions\n",
    "\n",
    "**Expected Improvements**:\n",
    "- Correct energy distribution (no more 4x too loud)\n",
    "- Better high-frequency content (above 4kHz)\n",
    "- Reduced edge artifacts\n",
    "- More natural violin timbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install librosa soundfile\n",
    "!pip install bigvgan  # Universal vocoder for music/speech\n",
    "\n",
    "# Import all required modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import soundfile as sf\n",
    "import os\n",
    "import bigvgan\n",
    "\n",
    "# Device configuration\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    SAMPLE_RATE = 22050\n",
    "    N_FFT = 1024\n",
    "    HOP_LENGTH = 256\n",
    "    N_MELS = 80  # Matches BigVGAN base model\n",
    "    CELL_CHANNELS = 16\n",
    "    UPDATE_STEPS_TRAIN = 96\n",
    "    UPDATE_STEPS_INFERENCE = 96\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 8000\n",
    "    LOG_INTERVAL = 100\n",
    "    OUTPUT_IMAGE_DIR = 'training_progress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_auditory_grid(audio_path, config):\n",
    "    \"\"\"Loads audio and converts to mel spectrogram.\"\"\"\n",
    "    waveform, sr = librosa.load(audio_path, sr=config.SAMPLE_RATE, mono=True)\n",
    "\n",
    "    # Ensure time dimension is multiple of 8\n",
    "    n_frames = len(waveform) / config.HOP_LENGTH\n",
    "    if n_frames % 8 != 0:\n",
    "        target_frames = int(np.ceil(n_frames / 8.0)) * 8\n",
    "        target_samples = target_frames * config.HOP_LENGTH\n",
    "        padding_needed = target_samples - len(waveform)\n",
    "        waveform = np.pad(waveform, (0, padding_needed), 'constant')\n",
    "\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=waveform, \n",
    "        sr=config.SAMPLE_RATE, \n",
    "        n_fft=config.N_FFT,\n",
    "        hop_length=config.HOP_LENGTH, \n",
    "        n_mels=config.N_MELS\n",
    "    )\n",
    "    return librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "def visualize_spectrogram(spectrogram, config, title='Mel-Spectrogram', output_path=None):\n",
    "    \"\"\"Visualizes a mel spectrogram.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    if isinstance(spectrogram, torch.Tensor):\n",
    "        spectrogram = spectrogram.detach().cpu().numpy()\n",
    "\n",
    "    librosa.display.specshow(\n",
    "        spectrogram, \n",
    "        sr=config.SAMPLE_RATE, \n",
    "        hop_length=config.HOP_LENGTH, \n",
    "        x_axis='time', \n",
    "        y_axis='mel'\n",
    "    )\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BigVGAN universal vocoder\n",
    "print(\"Loading BigVGAN universal vocoder...\")\n",
    "bigvgan_model = bigvgan.BigVGAN.from_pretrained(\n",
    "    'nvidia/bigvgan_base_22khz_80band',\n",
    "    use_cuda_kernel=False\n",
    ")\n",
    "bigvgan_model = bigvgan_model.to(device)\n",
    "bigvgan_model.eval()\n",
    "print(\"BigVGAN vocoder loaded successfully!\")\n",
    "print(f\"Sample rate: 22kHz, Mel bands: 80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNCAModel(nn.Module):\n",
    "    \"\"\"Textured Neural Cellular Automata with circular padding (FIXED).\"\"\"\n",
    "    def __init__(self, num_channels=16):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        perception_vector_size = self.num_channels * 4\n",
    "\n",
    "        self.update_mlp = nn.Sequential(\n",
    "            nn.Conv2d(perception_vector_size, 128, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, self.num_channels, 1, bias=True)\n",
    "        )\n",
    "        \n",
    "        # Initialize for stability\n",
    "        self.update_mlp[-1].weight.data.zero_()\n",
    "        self.update_mlp[-1].bias.data.zero_()\n",
    "        self.update_mlp[-1].bias.data[1] = 1.0  # Alpha channel bias\n",
    "\n",
    "    def perceive(self, grid):\n",
    "        \"\"\"Apply perception filters with CIRCULAR padding (FIX #4).\"\"\"\n",
    "        device = grid.device\n",
    "        \n",
    "        sobel_x = torch.tensor(\n",
    "            [[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
    "            dtype=torch.float32, device=device\n",
    "        ).view(1, 1, 3, 3)\n",
    "        sobel_y = sobel_x.transpose(-2, -1).contiguous()\n",
    "        laplacian = torch.tensor(\n",
    "            [[1, 2, 1], [2, -12, 2], [1, 2, 1]], \n",
    "            dtype=torch.float32, device=device\n",
    "        ).view(1, 1, 3, 3)\n",
    "\n",
    "        # Create kernels for each channel\n",
    "        sobel_x_kernel = sobel_x.repeat(self.num_channels, 1, 1, 1)\n",
    "        sobel_y_kernel = sobel_y.repeat(self.num_channels, 1, 1, 1)\n",
    "        laplacian_kernel = laplacian.repeat(self.num_channels, 1, 1, 1)\n",
    "\n",
    "        # Apply CIRCULAR padding to avoid edge artifacts\n",
    "        # Pad: (left, right, top, bottom)\n",
    "        grid_padded = F.pad(grid, (1, 1, 1, 1), mode='circular')\n",
    "\n",
    "        # Apply filters with padding=0 since we pre-padded\n",
    "        grad_x = F.conv2d(grid_padded, sobel_x_kernel, padding=0, groups=self.num_channels)\n",
    "        grad_y = F.conv2d(grid_padded, sobel_y_kernel, padding=0, groups=self.num_channels)\n",
    "        lap = F.conv2d(grid_padded, laplacian_kernel, padding=0, groups=self.num_channels)\n",
    "\n",
    "        return torch.cat([grid, grad_x, grad_y, lap], dim=1)\n",
    "\n",
    "    def forward(self, grid):\n",
    "        perception_vector = self.perceive(grid)\n",
    "        ds = self.update_mlp(perception_vector)\n",
    "        grid = grid + ds\n",
    "\n",
    "        # Clamp alpha channel\n",
    "        clamped_alpha = torch.clamp(grid[:, 1:2, :, :], 0.0, 1.0)\n",
    "        grid = torch.cat([grid[:, :1, :, :], clamped_alpha, grid[:, 2:, :, :]], dim=1)\n",
    "\n",
    "        # Apply living mask (also with circular padding)\n",
    "        alpha_channel = grid[:, 1:2, :, :]\n",
    "        alpha_padded = F.pad(alpha_channel, (1, 1, 1, 1), mode='circular')\n",
    "        living_mask = F.max_pool2d(alpha_padded, kernel_size=3, stride=1, padding=0)\n",
    "        grid = grid * living_mask\n",
    "\n",
    "        return grid\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"Perceptual loss based on Gram matrix.\"\"\"\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 7, 2, 3), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 5, 2, 2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.ReLU()\n",
    "        ).to(device)\n",
    "\n",
    "    def gram_matrix(self, f):\n",
    "        b, c, h, w = f.size()\n",
    "        f = f.view(b, c, h * w)\n",
    "        return torch.bmm(f, f.transpose(1, 2)) / (c * h * w)\n",
    "\n",
    "    def forward(self, gen_mel, target_mel):\n",
    "        return F.mse_loss(\n",
    "            self.gram_matrix(self.feature_extractor(gen_mel)),\n",
    "            self.gram_matrix(self.feature_extractor(target_mel))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, vocoder, loss_fn, optimizer, target_mel, config):\n",
    "    \"\"\"Train the TNCA model (FIXED loss weights).\"\"\"\n",
    "    print(\"\\n--- Starting Training (FIXED VERSION) ---\")\n",
    "    perceptual_loss_fn = loss_fn\n",
    "    l1_loss_fn = nn.L1Loss()\n",
    "\n",
    "    target_mel_tensor = torch.tensor(\n",
    "        target_mel, dtype=torch.float32\n",
    "    ).to(device).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "    # Mask for non-silent regions\n",
    "    target_mask = (target_mel_tensor > -70.0).float()\n",
    "\n",
    "    # FIX #2: INCREASED frequency weighting (1.0 -> 5.0 instead of 2.0)\n",
    "    n_mels = target_mel_tensor.shape[2]\n",
    "    frequency_loss_weights = torch.linspace(\n",
    "        1.0, 5.0, n_mels, device=device  # Changed from 2.0 to 5.0\n",
    "    ).view(1, 1, n_mels, 1)\n",
    "    print(f\"Frequency weighting: 1.0 (low freq) -> 5.0 (high freq)\")\n",
    "\n",
    "    # Initialize seed grid\n",
    "    seed_grid = torch.zeros(\n",
    "        1, config.CELL_CHANNELS, config.N_MELS, target_mel.shape[1], device=device\n",
    "    )\n",
    "    h, w = seed_grid.shape[2], seed_grid.shape[3]\n",
    "    seed_grid[:, 1, h//2, w//2] = 1.0\n",
    "\n",
    "    pbar = tqdm(range(config.NUM_EPOCHS), desc=\"Training...\")\n",
    "\n",
    "    for epoch in pbar:\n",
    "        grid = seed_grid.clone()\n",
    "        \n",
    "        # Evolve the grid\n",
    "        for _ in range(config.UPDATE_STEPS_TRAIN):\n",
    "            grid = model(grid)\n",
    "\n",
    "        # Extract and scale to dB range\n",
    "        generated_mel_db_unscaled = grid[:, 0:1, :, :]\n",
    "        generated_mel_db = torch.tanh(generated_mel_db_unscaled) * 40.0 - 40.0\n",
    "\n",
    "        # Calculate losses\n",
    "        p_loss = perceptual_loss_fn(\n",
    "            generated_mel_db * target_mask, \n",
    "            target_mel_tensor * target_mask\n",
    "        )\n",
    "        l1_loss = l1_loss_fn(\n",
    "            generated_mel_db * target_mask, \n",
    "            target_mel_tensor * target_mask\n",
    "        )\n",
    "        \n",
    "        # FIX #3: INCREASED L1 loss weight (0.1 -> 0.5)\n",
    "        loss = (2.0 * p_loss + 0.5 * l1_loss) * frequency_loss_weights  # Changed from 0.1\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if (epoch + 1) % config.LOG_INTERVAL == 0:\n",
    "            pbar.set_description(\n",
    "                f\"Epoch {epoch+1}, Loss: {loss.item():.6f} \"\n",
    "                f\"(P: {p_loss.item():.6f}, L1: {l1_loss.item():.6f})\"\n",
    "            )\n",
    "            filepath = os.path.join(\n",
    "                config.OUTPUT_IMAGE_DIR, \n",
    "                f'epoch_{(epoch+1):04d}.png'\n",
    "            )\n",
    "            visualize_spectrogram(\n",
    "                generated_mel_db.squeeze(), \n",
    "                config,\n",
    "                title=f'Generated Spectrogram - Epoch {epoch+1} (FIXED)',\n",
    "                output_path=filepath\n",
    "            )\n",
    "\n",
    "    print(f\"\\nTraining complete. Final loss: {loss.item():.6f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, vocoder, target_mel, config, output_path='generated_bigvgan_FIXED.wav'):\n",
    "    \"\"\"Generate audio (FIXED dB conversion).\"\"\"\n",
    "    print(\"\\n--- Running Inference with BigVGAN (FIXED) ---\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure target_mel is tensor on device\n",
    "    if isinstance(target_mel, np.ndarray):\n",
    "        target_mel = torch.tensor(target_mel, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        target_mel = target_mel.to(device)\n",
    "\n",
    "    # Initialize seed grid\n",
    "    seed_grid = torch.zeros(\n",
    "        1, config.CELL_CHANNELS, config.N_MELS, target_mel.shape[1], device=device\n",
    "    )\n",
    "    h, w = seed_grid.shape[2], seed_grid.shape[3]\n",
    "    seed_grid[:, 1, h//2, w//2] = 1.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        final_grid = seed_grid.clone()\n",
    "        \n",
    "        for _ in tqdm(range(config.UPDATE_STEPS_INFERENCE), desc=\"Generating...\"):\n",
    "            final_grid = model(final_grid)\n",
    "\n",
    "        # Extract and scale\n",
    "        final_mel_db_unscaled = final_grid[:, 0, :, :]\n",
    "        final_mel_db = torch.tanh(final_mel_db_unscaled) * 40.0 - 40.0\n",
    "\n",
    "        # FIX #1: CORRECT dB to power conversion\n",
    "        # librosa.power_to_db uses: dB = 10 * log10(power)\n",
    "        # Therefore: power = 10^(dB/10)\n",
    "        print(\"Converting dB to power spectrum (10^(dB/10))...\")\n",
    "        final_mel_power = torch.pow(10.0, final_mel_db / 10.0)  # FIXED: was /20, now /10\n",
    "        \n",
    "        print(f\"Mel spectrogram shape: {final_mel_power.shape}\")\n",
    "        print(f\"Mel power range: [{final_mel_power.min():.6f}, {final_mel_power.max():.6f}]\")\n",
    "\n",
    "        # BigVGAN expects [batch, mels, time]\n",
    "        if final_mel_power.dim() == 2:\n",
    "            final_mel_power = final_mel_power.unsqueeze(0)\n",
    "\n",
    "        print(\"Generating audio with BigVGAN vocoder...\")\n",
    "        \n",
    "        # BigVGAN inference\n",
    "        final_waveform = vocoder(final_mel_power)\n",
    "        \n",
    "        # BigVGAN returns [batch, 1, samples], squeeze to [samples]\n",
    "        final_waveform = final_waveform.squeeze()\n",
    "\n",
    "    # Save audio\n",
    "    waveform_np = final_waveform.cpu().numpy()\n",
    "    sf.write(output_path, waveform_np, config.SAMPLE_RATE)\n",
    "    \n",
    "    print(f\"\\nâœ… Inference complete!\")\n",
    "    print(f\"Audio saved to: {output_path}\")\n",
    "    print(f\"Audio duration: {len(waveform_np) / config.SAMPLE_RATE:.2f} seconds\")\n",
    "    print(f\"RMS energy: {np.sqrt(np.mean(waveform_np**2)):.4f}\")\n",
    "    \n",
    "    visualize_spectrogram(\n",
    "        final_mel_db.squeeze().cpu(), \n",
    "        config, \n",
    "        title='Final Generated Spectrogram (BigVGAN FIXED)'\n",
    "    )\n",
    "    \n",
    "    return waveform_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Load target audio\n",
    "target_audio_path = 'violin.wav'  # <-- Update with your file path\n",
    "\n",
    "if not os.path.exists(target_audio_path):\n",
    "    print(f\"ERROR: Audio file not found at '{target_audio_path}'\")\n",
    "    print(\"Please upload the file and update the path.\")\n",
    "else:\n",
    "    print(\"Loading target audio...\")\n",
    "    target_spectrogram = create_auditory_grid(target_audio_path, config)\n",
    "    \n",
    "    print(\"\\nTarget Spectrogram:\")\n",
    "    visualize_spectrogram(target_spectrogram, config, title='Target Spectrogram')\n",
    "\n",
    "    # Initialize models\n",
    "    print(\"\\nInitializing TNCA model (FIXED with circular padding)...\")\n",
    "    tnca_model = TNCAModel(config.CELL_CHANNELS).to(device)\n",
    "    perceptual_loss_fn = PerceptualLoss(device).to(device)\n",
    "    optimizer = optim.Adam(tnca_model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FIXES APPLIED:\")\n",
    "    print(\"  1. âœ… dB-to-power: Changed /20 to /10 (correct for power spectrograms)\")\n",
    "    print(\"  2. âœ… Frequency weights: Increased from 2.0 to 5.0 (emphasize high freq)\")\n",
    "    print(\"  3. âœ… L1 loss weight: Increased from 0.1 to 0.5 (capture more detail)\")\n",
    "    print(\"  4. âœ… Circular padding: Eliminate edge artifacts in CA\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Train\n",
    "    trained_model = train(\n",
    "        tnca_model, \n",
    "        bigvgan_model, \n",
    "        perceptual_loss_fn, \n",
    "        optimizer, \n",
    "        target_spectrogram, \n",
    "        config\n",
    "    )\n",
    "\n",
    "    # Generate audio with BigVGAN\n",
    "    generated_audio = inference(\n",
    "        trained_model, \n",
    "        bigvgan_model, \n",
    "        target_spectrogram, \n",
    "        config, \n",
    "        output_path='generated_bigvgan_FIXED.wav'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸŽ‰ TRAINING COMPLETE WITH FIXES!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nCompare 'generated_bigvgan_FIXED.wav' with 'generated_bigvgan-2.wav'\")\n",
    "    print(\"\\nExpected improvements:\")\n",
    "    print(\"  - Correct energy level (RMS ~0.10, not 0.42)\")\n",
    "    print(\"  - Better high-frequency content (above 4kHz)\")\n",
    "    print(\"  - No edge artifacts (vertical bands)\")\n",
    "    print(\"  - More natural violin timbre\")\n",
    "    print(\"\\nIf quality is good, proceed to Phase 2: Audio-domain loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
